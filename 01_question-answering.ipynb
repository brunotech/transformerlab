{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp question_answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering\n",
    "> Classes and functions for question answering tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the classes and functions defined in this module are adapted from the following resources:\n",
    "\n",
    "* [Sylvain Gugger's](https://twitter.com/GuggerSylvain?s=20) excellent [tutorial](https://github.com/huggingface/notebooks/blob/master/examples/question_answering.ipynb) on extractive question answering\n",
    "* The scripts and modules from the [question answering examples](https://github.com/huggingface/transformers/tree/master/examples/question-answering) in the `transformers` repository\n",
    "\n",
    "Compared to the results from HuggingFace's `run_qa.py` script, this implementation agrees to within 0.5% on the SQUAD v1 dataset:\n",
    "\n",
    "| Implementation | Exact Match | F1 |\n",
    "| :--- | :---: | :---: |\n",
    "| HuggingFace | 81.22 | 88.52 |\n",
    "| Ours | 80.82 | 88.22 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#hide\n",
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import datasets\n",
    "import transformers\n",
    "\n",
    "datasets.logging.set_verbosity_error()\n",
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import collections\n",
    "from typing import Union, List\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from transformers.trainer_utils import PredictionOutput\n",
    "from transformers.tokenization_utils import PreTrainedTokenizer\n",
    "from transformers import TrainingArguments, Trainer, EvalPrediction, default_data_collator\n",
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def prepare_train_features(examples: Union[str, List[str], List[List[str]]], tokenizer: PreTrainedTokenizer, pad_on_right: bool, max_length: int=384, doc_stride: int=128):\n",
    "    \"Tokenize and encode training examples in the SQuAD format\"\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        # label impossible answers with the index of the CLS token\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[\"answers\"][sample_index]\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "                token_start_index += 1\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "                token_end_index -= 1\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def prepare_validation_features(examples, tokenizer, pad_on_right, max_length, doc_stride):\n",
    "    \"Tokenize and encode validation examples in the SQuAD format\"\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples['question' if pad_on_right else 'context'],\n",
    "        examples['context' if pad_on_right else 'question'],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    tokenized_examples[\"example_id\"] = []\n",
    "\n",
    "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        context_index = 1 if pad_on_right else 0\n",
    "        sample_index = sample_mapping[i]\n",
    "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
    "        tokenized_examples[\"offset_mapping\"][i] = [\n",
    "            (o if sequence_ids[k] == context_index else None)\n",
    "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
    "        ]\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def convert_examples_to_features(dataset, tokenizer, num_train_examples, num_eval_examples, \n",
    "                                 max_length=384, doc_stride=128, seed=42):\n",
    "    \"Tokenize and encode the training and validation examples in the SQuAD format\"\n",
    "    max_length = max_length \n",
    "    doc_stride = doc_stride \n",
    "    pad_on_right = tokenizer.padding_side == \"right\"\n",
    "    fn_kwargs = {\n",
    "        \"tokenizer\": tokenizer,\n",
    "        \"max_length\": max_length,\n",
    "        \"doc_stride\": doc_stride,\n",
    "        \"pad_on_right\": pad_on_right\n",
    "    }\n",
    "    train_enc = (dataset['train']\n",
    "                 .shuffle(seed=seed)\n",
    "                 .select(range(num_train_examples))\n",
    "                 .map(prepare_train_features, fn_kwargs=fn_kwargs, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "                )\n",
    "    eval_enc = (dataset['validation']\n",
    "                .shuffle(seed=seed)\n",
    "                .select(range(num_eval_examples))\n",
    "                .map(prepare_validation_features, fn_kwargs=fn_kwargs, batched=True, remove_columns=dataset[\"validation\"].column_names)\n",
    "               )\n",
    "    eval_examples = dataset['validation'].shuffle(seed=seed).select(range(num_eval_examples))\n",
    "\n",
    "    return train_enc, eval_enc, eval_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "num_train_examples = 800\n",
    "num_eval_examples = 200\n",
    "squad_ds = load_dataset('squad')\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "train_ds, eval_ds, eval_examples = convert_examples_to_features(squad_ds, tokenizer, num_train_examples, num_eval_examples)\n",
    "assert eval_examples.num_rows == num_eval_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "metric = load_metric(\"squad\")\n",
    "\n",
    "def squad_metrics(p: EvalPrediction):\n",
    "    \"Compute the Exact Match and F1-score metrics on SQuAD\"\n",
    "    return metric.compute(predictions=p.predictions, references=p.label_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class QuestionAnsweringTrainingArguments(TrainingArguments):\n",
    "    def __init__(self, *args, max_length=384, doc_stride=128, version_2_with_negative=False, \n",
    "                 null_score_diff_threshold=0., n_best_size=20, max_answer_length=30,  **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        self.max_length = max_length\n",
    "        self.doc_stride = doc_stride\n",
    "        self.version_2_with_negative = version_2_with_negative\n",
    "        self.null_score_diff_threshold = null_score_diff_threshold\n",
    "        self.n_best_size = n_best_size\n",
    "        self.max_answer_length = max_answer_length\n",
    "        self.disable_tqdm = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class QuestionAnsweringTrainer(Trainer):\n",
    "    def __init__(self, *args, eval_examples=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.eval_examples = eval_examples\n",
    "        self.data_collator = default_data_collator\n",
    "        self.compute_metrics = squad_metrics\n",
    "        \n",
    "    def evaluate(self, eval_dataset=None, eval_examples=None, ignore_keys=None):\n",
    "        eval_dataset = self.eval_dataset if eval_dataset is None else eval_dataset\n",
    "        eval_dataloader = self.get_eval_dataloader(eval_dataset)\n",
    "        eval_examples = self.eval_examples if eval_examples is None else eval_examples\n",
    "\n",
    "        compute_metrics = self.compute_metrics\n",
    "        self.compute_metrics = None\n",
    "        try:\n",
    "            output = self.prediction_loop(\n",
    "                eval_dataloader,\n",
    "                description=\"Evaluation\",\n",
    "                prediction_loss_only=True if compute_metrics is None else None,\n",
    "                ignore_keys=ignore_keys,\n",
    "            )\n",
    "        finally:\n",
    "            self.compute_metrics = compute_metrics\n",
    "    \n",
    "        eval_dataset.set_format(type=eval_dataset.format[\"type\"], columns=list(eval_dataset.features.keys()))\n",
    "\n",
    "        if self.compute_metrics is not None:\n",
    "            eval_preds = self._post_process_function(eval_examples, eval_dataset, output.predictions)\n",
    "            metrics = self.compute_metrics(eval_preds)\n",
    "            # For some reason the eval_loss is not returned in output's metrics\n",
    "            # Work around since NotebookProgressCallback assumes eval_loss key exists\n",
    "            metrics['eval_loss'] = 'No log'\n",
    "\n",
    "            self.log(metrics)\n",
    "        else:\n",
    "            metrics = {}\n",
    "            \n",
    "        for key in list(metrics.keys()):\n",
    "            if not key.startswith(f\"eval_\"):\n",
    "                metrics[f\"eval_{key}\"] = metrics.pop(key)\n",
    "\n",
    "        self.control = self.callback_handler.on_evaluate(self.args, self.state, self.control, metrics)\n",
    "        return metrics\n",
    "\n",
    "    def predict(self, test_dataset, test_examples, ignore_keys=None):\n",
    "        test_dataloader = self.get_test_dataloader(test_dataset)\n",
    "        compute_metrics = self.compute_metrics\n",
    "        self.compute_metrics = None\n",
    "        try:\n",
    "            output = self.prediction_loop(\n",
    "                test_dataloader,\n",
    "                description=\"Evaluation\",\n",
    "                prediction_loss_only=True if compute_metrics is None else None,\n",
    "                ignore_keys=ignore_keys,\n",
    "            )\n",
    "        finally:\n",
    "            self.compute_metrics = compute_metrics\n",
    "\n",
    "        if self.compute_metrics is None:\n",
    "            return output\n",
    "\n",
    "        test_dataset.set_format(type=test_dataset.format[\"type\"], columns=list(test_dataset.features.keys()))\n",
    "        eval_preds = self._post_process_function(test_examples, test_dataset, output.predictions)\n",
    "        metrics = self.compute_metrics(eval_preds)\n",
    "\n",
    "        return PredictionOutput(predictions=eval_preds.predictions, label_ids=eval_preds.label_ids, metrics=metrics)\n",
    "    \n",
    "    \n",
    "    def _post_process_function(self, examples, features, predictions):\n",
    "        predictions = self._postprocess_qa_predictions(\n",
    "            examples=examples,\n",
    "            features=features,\n",
    "            predictions=predictions,\n",
    "            version_2_with_negative=self.args.version_2_with_negative,\n",
    "            n_best_size=self.args.n_best_size,\n",
    "            max_answer_length=self.args.max_answer_length,\n",
    "            null_score_diff_threshold=self.args.null_score_diff_threshold,\n",
    "            output_dir=self.args.output_dir,\n",
    "            is_world_process_zero=self.is_world_process_zero(),\n",
    "        )\n",
    "        if self.args.version_2_with_negative:\n",
    "            formatted_predictions = [\n",
    "                {\"id\": k, \"prediction_text\": v, \"no_answer_probability\": 0.0} for k, v in predictions.items()\n",
    "            ]\n",
    "        else:\n",
    "            formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in predictions.items()]\n",
    "        references = [{\"id\": ex[\"id\"], \"answers\": ex['answers']} for ex in self.eval_examples]\n",
    "        return EvalPrediction(predictions=formatted_predictions, label_ids=references)\n",
    "    \n",
    "    \n",
    "    def _postprocess_qa_predictions(\n",
    "        self,\n",
    "        examples,\n",
    "        features,\n",
    "        predictions,\n",
    "        version_2_with_negative= False,\n",
    "        n_best_size = None,\n",
    "        max_answer_length = None,\n",
    "        null_score_diff_threshold = None,\n",
    "        output_dir = None,\n",
    "        prefix = None,\n",
    "        is_world_process_zero = True,\n",
    "    ):\n",
    "        assert len(predictions) == 2, \"`predictions` should be a tuple with two elements (start_logits, end_logits).\"\n",
    "        all_start_logits, all_end_logits = predictions\n",
    "        assert len(predictions[0]) == len(features), f\"Got {len(predictions[0])} predictions and {len(features)} features.\"\n",
    "\n",
    "        example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "        features_per_example = collections.defaultdict(list)\n",
    "        for i, feature in enumerate(features):\n",
    "            features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
    "\n",
    "        all_predictions = collections.OrderedDict()\n",
    "\n",
    "        for example_index, example in enumerate(tqdm(examples)):\n",
    "            feature_indices = features_per_example[example_index]\n",
    "            min_null_prediction = None\n",
    "            prelim_predictions = []\n",
    "\n",
    "            for feature_index in feature_indices:\n",
    "                start_logits = all_start_logits[feature_index]\n",
    "                end_logits = all_end_logits[feature_index]\n",
    "                offset_mapping = features[feature_index][\"offset_mapping\"]\n",
    "                token_is_max_context = features[feature_index].get(\"token_is_max_context\", None)\n",
    "                feature_null_score = start_logits[0] + end_logits[0]\n",
    "                if min_null_prediction is None or min_null_prediction[\"score\"] > feature_null_score:\n",
    "                    min_null_prediction = {\n",
    "                        \"offsets\": (0, 0),\n",
    "                        \"score\": feature_null_score,\n",
    "                        \"start_logit\": start_logits[0],\n",
    "                        \"end_logit\": end_logits[0],\n",
    "                    }\n",
    "\n",
    "                start_indexes = np.argsort(start_logits)[-1 : -self.args.n_best_size - 1 : -1].tolist()\n",
    "                end_indexes = np.argsort(end_logits)[-1 : -self.args.n_best_size - 1 : -1].tolist()\n",
    "                for start_index in start_indexes:\n",
    "                    for end_index in end_indexes:\n",
    "                        if (\n",
    "                            start_index >= len(offset_mapping)\n",
    "                            or end_index >= len(offset_mapping)\n",
    "                            or offset_mapping[start_index] is None\n",
    "                            or offset_mapping[end_index] is None\n",
    "                        ):\n",
    "                            continue\n",
    "                        if end_index < start_index or end_index - start_index + 1 > self.args.max_answer_length:\n",
    "                            continue\n",
    "                        if token_is_max_context is not None and not token_is_max_context.get(str(start_index), False):\n",
    "                            continue\n",
    "                        prelim_predictions.append(\n",
    "                            {\n",
    "                                \"offsets\": (offset_mapping[start_index][0], offset_mapping[end_index][1]),\n",
    "                                \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                                \"start_logit\": start_logits[start_index],\n",
    "                                \"end_logit\": end_logits[end_index],\n",
    "                            }\n",
    "                        )\n",
    "            if self.args.version_2_with_negative:\n",
    "                prelim_predictions.append(min_null_prediction)\n",
    "                null_score = min_null_prediction[\"score\"]\n",
    "\n",
    "            predictions = sorted(prelim_predictions, key=lambda x: x[\"score\"], reverse=True)[:self.args.n_best_size]\n",
    "            if self.args.version_2_with_negative and not any(p[\"offsets\"] == (0, 0) for p in predictions):\n",
    "                predictions.append(min_null_prediction)\n",
    "\n",
    "            context = example[\"context\"]\n",
    "            for pred in predictions:\n",
    "                offsets = pred.pop(\"offsets\")\n",
    "                pred[\"text\"] = context[offsets[0] : offsets[1]]\n",
    "\n",
    "            if len(predictions) == 0 or (len(predictions) == 1 and predictions[0][\"text\"] == \"\"):\n",
    "                predictions.insert(0, {\"text\": \"empty\", \"start_logit\": 0.0, \"end_logit\": 0.0, \"score\": 0.0})\n",
    "\n",
    "            scores = np.array([pred.pop(\"score\") for pred in predictions])\n",
    "            exp_scores = np.exp(scores - np.max(scores))\n",
    "            probs = exp_scores / exp_scores.sum()\n",
    "\n",
    "            for prob, pred in zip(probs, predictions):\n",
    "                pred[\"probability\"] = prob\n",
    "\n",
    "            if not self.args.version_2_with_negative:\n",
    "                all_predictions[example[\"id\"]] = predictions[0][\"text\"]\n",
    "            else:\n",
    "                i = 0\n",
    "                while predictions[i][\"text\"] == \"\":\n",
    "                    i += 1\n",
    "                best_non_null_pred = predictions[i]\n",
    "\n",
    "                score_diff = null_score - best_non_null_pred[\"start_logit\"] - best_non_null_pred[\"end_logit\"]\n",
    "                if score_diff > self.args.null_score_diff_threshold:\n",
    "                    all_predictions[example[\"id\"]] = \"\"\n",
    "                else:\n",
    "                    all_predictions[example[\"id\"]] = best_non_null_pred[\"text\"]\n",
    "\n",
    "        return all_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example shows how the classes and functions in this module can be combined to fine-tune on the SQuAD v1 dataset. The first thing we need to do is grab the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "squad = load_dataset('squad')\n",
    "squad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each example, the key information is contained in the `context`, `question`, and `answer` fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answers': {'answer_start': [515], 'text': ['Saint Bernadette Soubirous']},\n",
       " 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
       " 'id': '5733be284776f41900661182',\n",
       " 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
       " 'title': 'University_of_Notre_Dame'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to tokenize and encode these texts. The following code does the job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "fn_kwargs = {\n",
    "    \"tokenizer\": tokenizer,\n",
    "    \"max_length\": 384,\n",
    "    \"doc_stride\": 128,\n",
    "    \"pad_on_right\": tokenizer.padding_side == \"right\"\n",
    "}\n",
    "train_ds = squad['train'].map(prepare_train_features, fn_kwargs=fn_kwargs, batched=True, remove_columns=squad[\"train\"].column_names)\n",
    "eval_ds = squad['validation'].map(prepare_validation_features, fn_kwargs=fn_kwargs, batched=True, remove_columns=squad[\"validation\"].column_names)\n",
    "eval_examples = squad['validation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to configure and instantiate the trainer using the same settings as those decribed in the `transformers` [examples](https://github.com/huggingface/transformers/tree/master/examples/question-answering#squad). We'll use the `model_init` argument to ensure that the model is initialised with the same random weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForQuestionAnswering\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on device: {device}\")\n",
    "\n",
    "def model_init():\n",
    "    return AutoModelForQuestionAnswering.from_pretrained(model_checkpoint).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we just need to specify the hyperparameters and data collator for padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import default_data_collator\n",
    "\n",
    "batch_size = 12\n",
    "learning_rate = 3e-5\n",
    "num_train_epochs = 2\n",
    "logging_steps = len(train_ds) // batch_size\n",
    "\n",
    "args = QuestionAnsweringTrainingArguments(\n",
    "    output_dir='checkpoints',\n",
    "    evaluation_strategy='epoch',\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    learning_rate=3e-5,\n",
    "    disable_tqdm=False,\n",
    "    logging_steps=logging_steps,\n",
    ")\n",
    "\n",
    "data_collator = default_data_collator\n",
    "\n",
    "trainer = QuestionAnsweringTrainer(\n",
    "    args=args,\n",
    "    model_init=model_init,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    eval_examples=eval_examples,\n",
    "    compute_metrics=squad_metrics,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and perform the fine-tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='14754' max='14754' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14754/14754 2:58:54, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Exact Match</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.266106</td>\n",
       "      <td>No log</td>\n",
       "      <td>79.309366</td>\n",
       "      <td>86.817847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.720876</td>\n",
       "      <td>No log</td>\n",
       "      <td>80.823084</td>\n",
       "      <td>88.228499</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51764c2b5e0c4af5b3436a3e6a243888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10570.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d152cdd31bc14cd5bf55585d114abe97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10570.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.train();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
