{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intent Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using transformers v4.1.1 and datasets v1.2.0\n",
      "Running on device: cuda\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datasets\n",
    "import transformers\n",
    "datasets.logging.set_verbosity_error()\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, \n",
    "                          TextClassificationPipeline, AdamW, get_linear_schedule_with_warmup)\n",
    "\n",
    "from transformerlab.pruning import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using transformers v{transformers.__version__} and datasets v{datasets.__version__}\")\n",
    "print(f\"Running on device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'intent'],\n",
       "        num_rows: 15250\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'intent'],\n",
       "        num_rows: 3100\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'intent'],\n",
       "        num_rows: 5500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clinc = load_dataset(\"clinc_oos\", \"plus\")\n",
    "clinc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinc.rename_column_(\"intent\", \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': 61,\n",
       " 'text': 'what expression would i use to say i love you if i were an italian'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clinc['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinc.set_format('pandas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>61</td>\n",
       "      <td>what expression would i use to say i love you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61</td>\n",
       "      <td>can you tell me how to say 'i do not speak muc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>61</td>\n",
       "      <td>what is the equivalent of, 'life is good' in f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61</td>\n",
       "      <td>tell me how to say, 'it is a beautiful morning...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>61</td>\n",
       "      <td>if i were mongolian, how would i say that i am...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   labels                                               text\n",
       "0      61  what expression would i use to say i love you ...\n",
       "1      61  can you tell me how to say 'i do not speak muc...\n",
       "2      61  what is the equivalent of, 'life is good' in f...\n",
       "3      61  tell me how to say, 'it is a beautiful morning...\n",
       "4      61  if i were mongolian, how would i say that i am..."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = clinc['train'][:]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([250, 100])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['labels'].value_counts().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['labels'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15000</th>\n",
       "      <td>42</td>\n",
       "      <td>how much is an overdraft fee for bank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15001</th>\n",
       "      <td>42</td>\n",
       "      <td>why are exponents preformed before multiplicat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15002</th>\n",
       "      <td>42</td>\n",
       "      <td>what size wipers does this car take</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15003</th>\n",
       "      <td>42</td>\n",
       "      <td>where is the dipstick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15004</th>\n",
       "      <td>42</td>\n",
       "      <td>how much is 1 share of aapl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15245</th>\n",
       "      <td>42</td>\n",
       "      <td>how can i get involved in yoga</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15246</th>\n",
       "      <td>42</td>\n",
       "      <td>is yoga healthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15247</th>\n",
       "      <td>42</td>\n",
       "      <td>what's the alma mater of the man that started ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15248</th>\n",
       "      <td>42</td>\n",
       "      <td>who has the most subscribers on youtube</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15249</th>\n",
       "      <td>42</td>\n",
       "      <td>do those i subscribe to have new videos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       labels                                               text\n",
       "15000      42              how much is an overdraft fee for bank\n",
       "15001      42  why are exponents preformed before multiplicat...\n",
       "15002      42                what size wipers does this car take\n",
       "15003      42                              where is the dipstick\n",
       "15004      42                        how much is 1 share of aapl\n",
       "...       ...                                                ...\n",
       "15245      42                     how can i get involved in yoga\n",
       "15246      42                                    is yoga healthy\n",
       "15247      42  what's the alma mater of the man that started ...\n",
       "15248      42            who has the most subscribers on youtube\n",
       "15249      42            do those i subscribe to have new videos\n",
       "\n",
       "[250 rows x 2 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['labels'] == 42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinc.reset_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score = load_metric('accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    predictions, labels = pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy_score.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 151"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune BERT-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bl_ckpt = \"bert-large-uncased-whole-word-masking\"\n",
    "bl_tokenizer = AutoTokenizer.from_pretrained(bl_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_encode(x, tokenizer): return tokenizer(x['text'], truncation=True)\n",
    "\n",
    "clinc_enc = clinc.map(tokenize_and_encode, fn_kwargs={'tokenizer' : bl_tokenizer}, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bl_model = AutoModelForSequenceClassification.from_pretrained(bl_ckpt, num_labels=num_labels).to(device)\n",
    "\n",
    "batch_size = 64\n",
    "learning_rate = 2e-5\n",
    "num_train_epochs = 3\n",
    "logging_steps = len(clinc_enc['train']) // batch_size\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir='checkpoints',\n",
    "    evaluation_strategy='epoch',\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=logging_steps,\n",
    "    disable_tqdm=False\n",
    ")\n",
    "\n",
    "bl_trainer = Trainer(\n",
    "    args=args,\n",
    "    model= bl_model,\n",
    "    train_dataset=clinc_enc['train'],\n",
    "    eval_dataset=clinc_enc['validation'],\n",
    "    tokenizer=bl_tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='98' max='49' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [49/49 02:58]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 5.149877548217773, 'eval_accuracy': 0.0064516129032258064}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bl_trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='717' max='717' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [717/717 09:13, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.028045</td>\n",
       "      <td>1.064417</td>\n",
       "      <td>0.905806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.689029</td>\n",
       "      <td>0.384993</td>\n",
       "      <td>0.951613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.290054</td>\n",
       "      <td>0.303301</td>\n",
       "      <td>0.954839</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=717, training_loss=1.3311438547184944)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bl_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bl_trainer.save_model(\"models/bert-large-uncased-wwm-finetuned-clinc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune BERT-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_ckpt = \"bert-base-uncased\"\n",
    "bb_tokenizer = AutoTokenizer.from_pretrained(bb_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init(): \n",
    "    return AutoModelForSequenceClassification.from_pretrained(bb_ckpt, num_labels=num_labels).to(device)\n",
    "\n",
    "batch_size = 64\n",
    "learning_rate = 2e-5\n",
    "num_train_epochs = 6\n",
    "logging_steps = len(clinc_enc['train']) // batch_size\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir='checkpoints',\n",
    "    evaluation_strategy='epoch',\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=logging_steps,\n",
    "    disable_tqdm=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    args=args,\n",
    "    model_init= model_init,\n",
    "    train_dataset=clinc_enc['train'],\n",
    "    eval_dataset=clinc_enc['validation'],\n",
    "    tokenizer=bb_tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='1434' max='1434' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1434/1434 06:02, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.428613</td>\n",
       "      <td>3.549596</td>\n",
       "      <td>0.603226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.973511</td>\n",
       "      <td>2.299460</td>\n",
       "      <td>0.858065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.980071</td>\n",
       "      <td>1.550097</td>\n",
       "      <td>0.915484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.365313</td>\n",
       "      <td>1.127650</td>\n",
       "      <td>0.935484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.027216</td>\n",
       "      <td>0.923742</td>\n",
       "      <td>0.941613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.872311</td>\n",
       "      <td>0.860549</td>\n",
       "      <td>0.942903</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1434, training_loss=2.1027118009170893)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"models/bert-base-uncased-finetuned-clinc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbert_ckpt = \"distilbert-base-uncased\"\n",
    "dbert_tokenizer = AutoTokenizer.from_pretrained(dbert_ckpt)\n",
    "\n",
    "clinc_enc = clinc.map(tokenize_and_encode, fn_kwargs={'tokenizer' : dbert_tokenizer}, batched=True)\n",
    "\n",
    "def model_init(): \n",
    "    return AutoModelForSequenceClassification.from_pretrained(dbert_ckpt, num_labels=num_labels).to(device)\n",
    "\n",
    "batch_size = 64\n",
    "learning_rate = 2e-5\n",
    "num_train_epochs = 6\n",
    "logging_steps = len(clinc_enc['train']) // batch_size\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir='checkpoints',\n",
    "    evaluation_strategy='epoch',\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=logging_steps,\n",
    "    disable_tqdm=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    args=args,\n",
    "    model_init= model_init,\n",
    "    train_dataset=clinc_enc['train'],\n",
    "    eval_dataset=clinc_enc['validation'],\n",
    "    tokenizer=dbert_tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='1434' max='1434' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1434/1434 03:15, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.430927</td>\n",
       "      <td>3.567706</td>\n",
       "      <td>0.678387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.968706</td>\n",
       "      <td>2.220834</td>\n",
       "      <td>0.823871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.869094</td>\n",
       "      <td>1.402024</td>\n",
       "      <td>0.877097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.229911</td>\n",
       "      <td>0.984138</td>\n",
       "      <td>0.906452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.891678</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.914839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.749713</td>\n",
       "      <td>0.740458</td>\n",
       "      <td>0.919677</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1434, training_loss=2.017932115738362)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"models/distilbert-base-uncased-finetuned-clinc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distill from BERT-large to BERT-base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main thing we need to implement task-specific distillation is augment the standard cross-entropy loss with a distillation term (see above equation). We can implement this by overriding the `compute_loss` method of the `QuestionAnsweringTrainer`, but first let's define the training arguments we'll need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistillationTrainingArguments(TrainingArguments):\n",
    "    def __init__(self, *args, alpha_ce=0.5, alpha_distil=0.5, temperature=2.0, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        self.alpha_ce = alpha_ce\n",
    "        self.alpha_distil = alpha_distil\n",
    "        self.temperature = temperature\n",
    "        self.disable_tqdm = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the trainer, we'll need a few ingredients:\n",
    "\n",
    "* We need two models (a teacher and student), and since the `model` attribute is the one that is optimized, we'll just add an attribute for the teacher\n",
    "* When we pass the question and context to the student or teacher, we get a range of scores (logits) for the start and end positions. Since we want to minimize the distance between the teacher and student predictions , we'll use the KL-divergence as our distillation loss\n",
    "* Once the distillation loss is computed, we take a linear combination with the cross-entropy to obtain our final loss function\n",
    "\n",
    "The following code does the trick:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistillationTrainer(Trainer):\n",
    "    def __init__(self, *args, teacher_model=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.teacher = teacher_model\n",
    "        self.teacher.eval()\n",
    "        self.train_dataset.set_format(\n",
    "            type=self.train_dataset.format[\"type\"], columns=list(self.train_dataset.features.keys()))\n",
    "\n",
    "    def compute_loss(self, model, inputs):\n",
    "        inputs_stu = {\n",
    "            \"input_ids\": inputs['input_ids'],\n",
    "            \"attention_mask\": inputs['attention_mask'],\n",
    "            \"labels\": inputs['labels']\n",
    "            }\n",
    "        if \"token_type_ids\" in inputs:\n",
    "            inputs_stu['token_type_ids'] = inputs['token_type_ids']\n",
    "        outputs_stu = model(**inputs_stu)\n",
    "        loss = outputs_stu.loss\n",
    "        logits_stu = outputs_stu.logits\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs_tea = self.teacher(\n",
    "                input_ids=inputs[\"input_ids\"], \n",
    "                token_type_ids=inputs[\"token_type_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                labels=inputs[\"labels\"])\n",
    "            logits_tea = outputs_tea.logits\n",
    "        assert logits_tea.size() == logits_stu.size()\n",
    "        \n",
    "        loss_fct = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "        loss_logits = (loss_fct(\n",
    "            F.log_softmax(logits_stu / self.args.temperature, dim=-1),\n",
    "            F.softmax(logits_tea / self.args.temperature, dim=-1)) * (self.args.temperature ** 2))\n",
    "        loss = self.args.alpha_distil * loss_logits + self.args.alpha_ce * loss\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's then a similar process to configure and initialise the trainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "logging_steps = len(clinc_enc['train']) // batch_size\n",
    "\n",
    "student_training_args = DistillationTrainingArguments(\n",
    "    output_dir=f\"checkpoints\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=6,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=logging_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_checkpoint = \"models/bert-large-uncased-wwm-finetuned-clinc150\"\n",
    "student_checkpoint = \"bert-base-uncased\"\n",
    "teacher_model = AutoModelForSequenceClassification.from_pretrained(teacher_checkpoint, num_labels=num_labels).to(device)\n",
    "\n",
    "def student_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(student_checkpoint, num_labels=num_labels).to(device)\n",
    "\n",
    "student_tokenizer = AutoTokenizer.from_pretrained(student_checkpoint)\n",
    "\n",
    "distil_trainer = DistillationTrainer(\n",
    "    model_init=student_init,\n",
    "    teacher_model=teacher_model,\n",
    "    args=student_training_args,\n",
    "    train_dataset=clinc_enc['train'],\n",
    "    eval_dataset=clinc_enc['validation'],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=student_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='98' max='49' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [49/49 01:39]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 5.067701816558838, 'eval_accuracy': 0.005483870967741935}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distil_trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='1434' max='1434' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1434/1434 10:18, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.354883</td>\n",
       "      <td>3.548548</td>\n",
       "      <td>0.622581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.252753</td>\n",
       "      <td>2.219483</td>\n",
       "      <td>0.828710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.514512</td>\n",
       "      <td>1.468975</td>\n",
       "      <td>0.890968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.081530</td>\n",
       "      <td>1.065988</td>\n",
       "      <td>0.917097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.839902</td>\n",
       "      <td>0.876521</td>\n",
       "      <td>0.920645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.733032</td>\n",
       "      <td>0.816523</td>\n",
       "      <td>0.928710</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1434, training_loss=1.6256008710156258)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distil_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 60:40 teacher / student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_distill_ratio(r=0.5):\n",
    "    distil_trainer.args.alpha_distil = r\n",
    "    distil_trainer.args.alpha_ce = 1 - r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_distill_ratio(0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='1434' max='1434' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1434/1434 10:15, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.070265</td>\n",
       "      <td>3.434687</td>\n",
       "      <td>0.656774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.070854</td>\n",
       "      <td>2.180539</td>\n",
       "      <td>0.859677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.414529</td>\n",
       "      <td>1.447803</td>\n",
       "      <td>0.905806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.014256</td>\n",
       "      <td>1.041266</td>\n",
       "      <td>0.929355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.791487</td>\n",
       "      <td>0.849802</td>\n",
       "      <td>0.937742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.689871</td>\n",
       "      <td>0.791683</td>\n",
       "      <td>0.942258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1434, training_loss=1.5051100949030707)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distil_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 70:30 teacher / student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_distill_ratio(0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='1434' max='1434' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1434/1434 10:11, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.852746</td>\n",
       "      <td>3.416343</td>\n",
       "      <td>0.653548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.920609</td>\n",
       "      <td>2.159794</td>\n",
       "      <td>0.848387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.317353</td>\n",
       "      <td>1.435072</td>\n",
       "      <td>0.898710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.949080</td>\n",
       "      <td>1.032846</td>\n",
       "      <td>0.924839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.742410</td>\n",
       "      <td>0.842547</td>\n",
       "      <td>0.933871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.648488</td>\n",
       "      <td>0.785621</td>\n",
       "      <td>0.938065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1434, training_loss=1.4019301113889473)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distil_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distil_trainer.save_model('models/bert-base-uncased-distilled-clinc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distill from BERT-base to DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "logging_steps = len(clinc_enc['train']) // batch_size\n",
    "\n",
    "student_training_args = DistillationTrainingArguments(\n",
    "    output_dir=f\"checkpoints\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=6,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=logging_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_checkpoint = \"models/bert-base-uncased-finetuned-clinc\"\n",
    "student_checkpoint = \"distilbert-base-uncased\"\n",
    "teacher_model = AutoModelForSequenceClassification.from_pretrained(teacher_checkpoint, num_labels=num_labels).to(device)\n",
    "\n",
    "def student_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(student_checkpoint, num_labels=num_labels).to(device)\n",
    "\n",
    "student_tokenizer = AutoTokenizer.from_pretrained(student_checkpoint)\n",
    "\n",
    "distil_trainer = DistillationTrainer(\n",
    "    model_init=student_init,\n",
    "    teacher_model=teacher_model,\n",
    "    args=student_training_args,\n",
    "    train_dataset=clinc_enc['train'],\n",
    "    eval_dataset=clinc_enc['validation'],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=student_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distil_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speed test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a simple benchmark, here we compare the time it takes for our teacher and student to generate 1,000 predictions on a CPU (to simulate a production environment). First, we load our fine-tuned models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_model_ckpt = 'models/bert-base-uncased-distilled-clinc150'\n",
    "teacher_model_ckpt = 'models/bert-large-uncased-wwm-finetuned-clinc150'\n",
    "\n",
    "student_tokenizer = AutoTokenizer.from_pretrained(student_model_ckpt)\n",
    "student_model = AutoModelForSequenceClassification.from_pretrained(student_model_ckpt, num_labels=num_labels).to('cpu')\n",
    "\n",
    "teacher_tokenizer = AutoTokenizer.from_pretrained(teacher_model_ckpt)\n",
    "teacher_model = AutoModelForSequenceClassification.from_pretrained(teacher_model_ckpt, num_labels=num_labels).to('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create two pipelines for the student and teacher:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_pipe = TextClassificationPipeline(model=student_model, tokenizer=student_tokenizer)\n",
    "teacher_pipe = TextClassificationPipeline(model=teacher_model, tokenizer=teacher_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then run the inference test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39min, sys: 26.3 s, total: 39min 26s\n",
      "Wall time: 5min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for idx in range(100):\n",
    "    teacher_pipe(clinc['test'][idx]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25min 57s, sys: 9.58 s, total: 26min 7s\n",
      "Wall time: 4min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for idx in range(100):\n",
    "    student_pipe(clinc['test'][idx]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this example, we see roughly a 2x speedup from using a distilled model with less than 3% drop in Exact Match / F1-score!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movement Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PruningTrainingArguments(TrainingArguments):\n",
    "    def __init__(self, *args, initial_threshold=1., final_threshold=0.1, initial_warmup=1, final_warmup=2, final_lambda=0.,\n",
    "                 mask_scores_learning_rate=0., **kwargs): \n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.initial_threshold = initial_threshold\n",
    "        self.final_threshold = final_threshold\n",
    "        self.initial_warmup = initial_warmup\n",
    "        self.final_warmup = final_warmup\n",
    "        self.final_lambda = final_lambda\n",
    "        self.mask_scores_learning_rate = mask_scores_learning_rate\n",
    "        self.disable_tqdm = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PruningTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        if self.args.max_steps > 0:\n",
    "            self.t_total = self.args.max_steps\n",
    "            self.args.num_train_epochs = self.args.max_steps // (len(self.get_train_dataloader()) // self.args.gradient_accumulation_steps) + 1\n",
    "        else:\n",
    "            self.t_total = len(self.get_train_dataloader()) // self.args.gradient_accumulation_steps * self.args.num_train_epochs\n",
    "            \n",
    "        \n",
    "    def create_optimizer_and_scheduler(self, num_training_steps: int):\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in self.model.named_parameters() if \"mask_score\" in n and p.requires_grad],\n",
    "                \"lr\": self.args.mask_scores_learning_rate,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p\n",
    "                    for n, p in self.model.named_parameters()\n",
    "                    if \"mask_score\" not in n and p.requires_grad and not any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"lr\": self.args.learning_rate,\n",
    "                \"weight_decay\": self.args.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p\n",
    "                    for n, p in self.model.named_parameters()\n",
    "                    if \"mask_score\" not in n and p.requires_grad and any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"lr\": self.args.learning_rate,\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        self.optimizer = AdamW(optimizer_grouped_parameters, lr=self.args.learning_rate, eps=self.args.adam_epsilon)\n",
    "        self.lr_scheduler = get_linear_schedule_with_warmup(\n",
    "            self.optimizer, num_warmup_steps=self.args.warmup_steps, num_training_steps=self.t_total\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def compute_loss(self, model, inputs):\n",
    "            \n",
    "        threshold, regu_lambda = self._schedule_threshold(\n",
    "            step=self.state.global_step+1,\n",
    "            total_step=self.t_total,\n",
    "            warmup_steps=self.args.warmup_steps,\n",
    "            final_threshold=self.args.final_threshold,\n",
    "            initial_threshold=self.args.initial_threshold,\n",
    "            final_warmup=self.args.final_warmup,\n",
    "            initial_warmup=self.args.initial_warmup,\n",
    "            final_lambda=self.args.final_lambda,\n",
    "        )\n",
    "        inputs[\"threshold\"] = threshold  \n",
    "        outputs = model(**inputs)\n",
    "        loss, logits = outputs\n",
    "        return loss\n",
    "    \n",
    "    def _schedule_threshold(\n",
    "        self,\n",
    "        step: int,\n",
    "        total_step: int,\n",
    "        warmup_steps: int,\n",
    "        initial_threshold: float,\n",
    "        final_threshold: float,\n",
    "        initial_warmup: int,\n",
    "        final_warmup: int,\n",
    "        final_lambda: float,\n",
    "    ):\n",
    "        if step <= initial_warmup * warmup_steps:\n",
    "            threshold = initial_threshold\n",
    "        elif step > (total_step - final_warmup * warmup_steps):\n",
    "            threshold = final_threshold\n",
    "        else:\n",
    "            spars_warmup_steps = initial_warmup * warmup_steps\n",
    "            spars_schedu_steps = (final_warmup + initial_warmup) * warmup_steps\n",
    "            mul_coeff = 1 - (step - spars_warmup_steps) / (total_step - spars_schedu_steps)\n",
    "            threshold = final_threshold + (initial_threshold - final_threshold) * (mul_coeff ** 3)\n",
    "        regu_lambda = final_lambda * threshold / final_threshold\n",
    "        return threshold, regu_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "\n",
    "masked_config = MaskedBertConfig(pruning_method='topK', mask_init='constant', mask_scale=0., num_labels=num_labels)\n",
    "\n",
    "def model_init():\n",
    "    return MaskedBertForSequenceClassification.from_pretrained(model_ckpt, config=masked_config).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we're using a `model_init` function so that we can perform multiple runs wih the same trainer. Next we specify the hyperparameter that will be fixed across each run:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0% pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "logging_steps = len(clinc_enc['train']) // batch_size\n",
    "\n",
    "# pruning params\n",
    "initial_threshold = 1.\n",
    "final_threshold = 1\n",
    "initial_warmup = 1\n",
    "final_warmup = 2\n",
    "final_lambda = 0\n",
    "num_train_epochs = 6\n",
    "warmup_steps = 0 #logging_steps * num_train_epochs * 0.1\n",
    "mask_scores_learning_rate = 0 #1e-2\n",
    "\n",
    "pruning_training_args = PruningTrainingArguments(\n",
    "    output_dir=\"checkpoints\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=3e-5, # reduce?\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    logging_steps=logging_steps,\n",
    "    initial_threshold=initial_threshold,\n",
    "    final_threshold=final_threshold,\n",
    "    initial_warmup=initial_warmup,\n",
    "    final_warmup=final_warmup,\n",
    "    final_lambda=final_lambda,\n",
    "    warmup_steps=warmup_steps,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    mask_scores_learning_rate=mask_scores_learning_rate,\n",
    "    weight_decay=0.01\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to do this manually for now!\n",
    "train_ds = clinc_enc['train']\n",
    "eval_ds = clinc_enc['validation'].map(lambda x : {'threshold': final_threshold})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruning_trainer = PruningTrainer(\n",
    "    model_init=model_init,\n",
    "    args=pruning_training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='1434' max='1434' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1434/1434 14:35, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.184197</td>\n",
       "      <td>2.965764</td>\n",
       "      <td>0.721290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.223854</td>\n",
       "      <td>1.439246</td>\n",
       "      <td>0.920323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.080129</td>\n",
       "      <td>0.748266</td>\n",
       "      <td>0.945484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.545227</td>\n",
       "      <td>0.471417</td>\n",
       "      <td>0.952903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.326512</td>\n",
       "      <td>0.372631</td>\n",
       "      <td>0.954839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.240158</td>\n",
       "      <td>0.346093</td>\n",
       "      <td>0.955806</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1434, training_loss=1.4284142205904717)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruning_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 90% weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "logging_steps = len(clinc_enc['train']) // batch_size\n",
    "\n",
    "# pruning params\n",
    "initial_threshold = 1.\n",
    "final_threshold = 0.9\n",
    "initial_warmup = 1\n",
    "final_warmup = 2\n",
    "final_lambda = 0\n",
    "num_train_epochs = 10\n",
    "warmup_steps = logging_steps * num_train_epochs * 0.1\n",
    "mask_scores_learning_rate = 1e-2\n",
    "\n",
    "pruning_training_args = PruningTrainingArguments(\n",
    "    output_dir=\"checkpoints\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    logging_steps=logging_steps,\n",
    "    initial_threshold=initial_threshold,\n",
    "    final_threshold=final_threshold,\n",
    "    initial_warmup=initial_warmup,\n",
    "    final_warmup=final_warmup,\n",
    "    final_lambda=final_lambda,\n",
    "    warmup_steps=warmup_steps,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    mask_scores_learning_rate=mask_scores_learning_rate,\n",
    "    weight_decay=0.01\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to do this manually for now!\n",
    "train_ds = clinc_enc['train']\n",
    "eval_ds = clinc_enc['validation'].map(lambda x : {'threshold': final_threshold})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruning_trainer = PruningTrainer(\n",
    "    model_init=model_init,\n",
    "    args=pruning_training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='2390' max='2390' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2390/2390 28:48, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.870851</td>\n",
       "      <td>5.123807</td>\n",
       "      <td>0.006452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.779082</td>\n",
       "      <td>2.396365</td>\n",
       "      <td>0.752258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.001543</td>\n",
       "      <td>0.834310</td>\n",
       "      <td>0.913226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.421721</td>\n",
       "      <td>0.543108</td>\n",
       "      <td>0.919677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.221068</td>\n",
       "      <td>0.408917</td>\n",
       "      <td>0.939032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.137422</td>\n",
       "      <td>0.368939</td>\n",
       "      <td>0.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.091896</td>\n",
       "      <td>0.318676</td>\n",
       "      <td>0.946129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.070028</td>\n",
       "      <td>0.330230</td>\n",
       "      <td>0.944839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.058563</td>\n",
       "      <td>0.311045</td>\n",
       "      <td>0.949355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.053597</td>\n",
       "      <td>0.310240</td>\n",
       "      <td>0.949355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2390, training_loss=0.9667764479645127)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruning_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruning_trainer.save_model(\"models/prunebert-base-uncased-90-finetuned-clinc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 70% weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "logging_steps = len(clinc_enc['train']) // batch_size\n",
    "\n",
    "# pruning params\n",
    "initial_threshold = 1.\n",
    "final_threshold = 0.7\n",
    "initial_warmup = 1\n",
    "final_warmup = 2\n",
    "final_lambda = 0\n",
    "num_train_epochs = 10\n",
    "warmup_steps = logging_steps * num_train_epochs * 0.1\n",
    "mask_scores_learning_rate = 1e-2\n",
    "\n",
    "pruning_training_args = PruningTrainingArguments(\n",
    "    output_dir=\"checkpoints\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    logging_steps=logging_steps,\n",
    "    initial_threshold=initial_threshold,\n",
    "    final_threshold=final_threshold,\n",
    "    initial_warmup=initial_warmup,\n",
    "    final_warmup=final_warmup,\n",
    "    final_lambda=final_lambda,\n",
    "    warmup_steps=warmup_steps,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    mask_scores_learning_rate=mask_scores_learning_rate,\n",
    "    weight_decay=0.01\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to do this manually for now!\n",
    "train_ds = clinc_enc['train']\n",
    "eval_ds = clinc_enc['validation'].map(lambda x : {'threshold': final_threshold})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruning_trainer = PruningTrainer(\n",
    "    model_init=model_init,\n",
    "    args=pruning_training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='2390' max='2390' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2390/2390 28:58, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.870851</td>\n",
       "      <td>5.095470</td>\n",
       "      <td>0.003226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.681642</td>\n",
       "      <td>3.792091</td>\n",
       "      <td>0.316129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.095286</td>\n",
       "      <td>1.268739</td>\n",
       "      <td>0.832258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.608820</td>\n",
       "      <td>0.734605</td>\n",
       "      <td>0.890000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.391964</td>\n",
       "      <td>0.615032</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.259445</td>\n",
       "      <td>0.557274</td>\n",
       "      <td>0.906774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.178657</td>\n",
       "      <td>0.462829</td>\n",
       "      <td>0.924839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.127867</td>\n",
       "      <td>0.431092</td>\n",
       "      <td>0.930968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.089605</td>\n",
       "      <td>0.375977</td>\n",
       "      <td>0.938710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.073907</td>\n",
       "      <td>0.373035</td>\n",
       "      <td>0.938387</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2390, training_loss=1.0338097955392493)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruning_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruning_trainer.save_model(\"models/prunebert-base-uncased-70-finetuned-clinc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 50% weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "logging_steps = len(clinc_enc['train']) // batch_size\n",
    "\n",
    "# pruning params\n",
    "initial_threshold = 1.\n",
    "final_threshold = 0.5\n",
    "initial_warmup = 1\n",
    "final_warmup = 2\n",
    "final_lambda = 0\n",
    "num_train_epochs = 10\n",
    "warmup_steps = logging_steps * num_train_epochs * 0.1\n",
    "mask_scores_learning_rate = 1e-2\n",
    "\n",
    "pruning_training_args = PruningTrainingArguments(\n",
    "    output_dir=\"checkpoints\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    logging_steps=logging_steps,\n",
    "    initial_threshold=initial_threshold,\n",
    "    final_threshold=final_threshold,\n",
    "    initial_warmup=initial_warmup,\n",
    "    final_warmup=final_warmup,\n",
    "    final_lambda=final_lambda,\n",
    "    warmup_steps=warmup_steps,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    mask_scores_learning_rate=mask_scores_learning_rate,\n",
    "    weight_decay=0.01\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to do this manually for now!\n",
    "train_ds = clinc_enc['train']\n",
    "eval_ds = clinc_enc['validation'].map(lambda x : {'threshold': final_threshold})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruning_trainer = PruningTrainer(\n",
    "    model_init=model_init,\n",
    "    args=pruning_training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='2390' max='2390' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2390/2390 29:03, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.870851</td>\n",
       "      <td>5.135061</td>\n",
       "      <td>0.006774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.746694</td>\n",
       "      <td>3.625696</td>\n",
       "      <td>0.417742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.297235</td>\n",
       "      <td>1.415797</td>\n",
       "      <td>0.815161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.778661</td>\n",
       "      <td>0.848564</td>\n",
       "      <td>0.861613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.535028</td>\n",
       "      <td>0.670781</td>\n",
       "      <td>0.879677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.358197</td>\n",
       "      <td>0.580886</td>\n",
       "      <td>0.903871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.250263</td>\n",
       "      <td>0.518113</td>\n",
       "      <td>0.912903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.166789</td>\n",
       "      <td>0.483631</td>\n",
       "      <td>0.919355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.116787</td>\n",
       "      <td>0.447267</td>\n",
       "      <td>0.923871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.088873</td>\n",
       "      <td>0.423120</td>\n",
       "      <td>0.929355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2390, training_loss=1.1166658250357815)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruning_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruning_trainer.save_model(\"models/prunebert-base-uncased-50-finetuned-clinc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 30% weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "logging_steps = len(clinc_enc['train']) // batch_size\n",
    "\n",
    "# pruning params\n",
    "initial_threshold = 1.\n",
    "final_threshold = 0.3\n",
    "initial_warmup = 1\n",
    "final_warmup = 2\n",
    "final_lambda = 0\n",
    "num_train_epochs = 10\n",
    "warmup_steps = logging_steps * num_train_epochs * 0.1\n",
    "mask_scores_learning_rate = 1e-2\n",
    "\n",
    "pruning_training_args = PruningTrainingArguments(\n",
    "    output_dir=\"checkpoints\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    logging_steps=logging_steps,\n",
    "    initial_threshold=initial_threshold,\n",
    "    final_threshold=final_threshold,\n",
    "    initial_warmup=initial_warmup,\n",
    "    final_warmup=final_warmup,\n",
    "    final_lambda=final_lambda,\n",
    "    warmup_steps=warmup_steps,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    mask_scores_learning_rate=mask_scores_learning_rate,\n",
    "    weight_decay=0.01\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to do this manually for now!\n",
    "train_ds = clinc_enc['train']\n",
    "eval_ds = clinc_enc['validation'].map(lambda x : {'threshold': final_threshold})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruning_trainer = PruningTrainer(\n",
    "    model_init=model_init,\n",
    "    args=pruning_training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='2390' max='2390' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2390/2390 29:01, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.870851</td>\n",
       "      <td>5.142257</td>\n",
       "      <td>0.001935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.859436</td>\n",
       "      <td>4.495208</td>\n",
       "      <td>0.103871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.440854</td>\n",
       "      <td>2.592907</td>\n",
       "      <td>0.585161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.882373</td>\n",
       "      <td>1.055322</td>\n",
       "      <td>0.833871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.558920</td>\n",
       "      <td>0.709714</td>\n",
       "      <td>0.871290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.379529</td>\n",
       "      <td>0.596916</td>\n",
       "      <td>0.888387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.262780</td>\n",
       "      <td>0.484533</td>\n",
       "      <td>0.910968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.166430</td>\n",
       "      <td>0.434604</td>\n",
       "      <td>0.922258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.117576</td>\n",
       "      <td>0.400300</td>\n",
       "      <td>0.927097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.091225</td>\n",
       "      <td>0.394097</td>\n",
       "      <td>0.928710</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2390, training_loss=1.1585227120122152)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruning_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruning_trainer.save_model(\"models/prunebert-base-uncased-30-finetuned-clinc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10% weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "logging_steps = len(clinc_enc['train']) // batch_size\n",
    "\n",
    "# pruning params\n",
    "initial_threshold = 1.\n",
    "final_threshold = 0.1\n",
    "initial_warmup = 1\n",
    "final_warmup = 2\n",
    "final_lambda = 0\n",
    "num_train_epochs = 10\n",
    "warmup_steps = logging_steps * num_train_epochs * 0.1\n",
    "mask_scores_learning_rate = 1e-2\n",
    "\n",
    "pruning_training_args = PruningTrainingArguments(\n",
    "    output_dir=\"checkpoints\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    logging_steps=logging_steps,\n",
    "    initial_threshold=initial_threshold,\n",
    "    final_threshold=final_threshold,\n",
    "    initial_warmup=initial_warmup,\n",
    "    final_warmup=final_warmup,\n",
    "    final_lambda=final_lambda,\n",
    "    warmup_steps=warmup_steps,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    mask_scores_learning_rate=mask_scores_learning_rate,\n",
    "    weight_decay=0.01\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to do this manually for now!\n",
    "train_ds = clinc_enc['train']\n",
    "eval_ds = clinc_enc['validation'].map(lambda x : {'threshold': final_threshold})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruning_trainer = PruningTrainer(\n",
    "    model_init=model_init,\n",
    "    args=pruning_training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='2390' max='2390' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2390/2390 28:54, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.870851</td>\n",
       "      <td>5.037233</td>\n",
       "      <td>0.006452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.989605</td>\n",
       "      <td>5.098973</td>\n",
       "      <td>0.006452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.583201</td>\n",
       "      <td>5.098977</td>\n",
       "      <td>0.006774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.924001</td>\n",
       "      <td>5.037778</td>\n",
       "      <td>0.015484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.580629</td>\n",
       "      <td>3.960984</td>\n",
       "      <td>0.342258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.392310</td>\n",
       "      <td>0.735440</td>\n",
       "      <td>0.895161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.265442</td>\n",
       "      <td>0.451762</td>\n",
       "      <td>0.921935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.181168</td>\n",
       "      <td>0.386650</td>\n",
       "      <td>0.929032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.125114</td>\n",
       "      <td>0.371531</td>\n",
       "      <td>0.932258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.102001</td>\n",
       "      <td>0.356328</td>\n",
       "      <td>0.932903</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2390, training_loss=1.1968463150527189)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruning_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruning_trainer.save_model(\"models/prunebert-base-uncased-10-finetuned-clinc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
