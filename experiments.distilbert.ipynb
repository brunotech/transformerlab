{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DistilBERT\n",
    "> A partial reimplementation of DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter by Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf [[arXiv:1910.01108](https://arxiv.org/abs/1910.01108)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to explore _task-specific_ knowledge distillation, where a teacher is used to augment the cross-entropy loss of the student during fine-tuning:\n",
    "\n",
    "$${\\cal L}(\\mathbf{x}|T) = - \\sum_i \\bar{y}_i\\log y_i(\\mathbf{x}|T) -T^2 \\sum_i \\hat{y}_i(\\mathbf{x}|T)\\log y_i(\\mathbf{x}|T) \\,.$$\n",
    "\n",
    "Here $T$ is the temperature, $\\hat{y}$ are the outputs from the model, $\\bar{y}$ the ground-truth labels, and $y_i$ a softmax with temperature.\n",
    "\n",
    "This neat idea comes from the DistilBERT paper, where the authors found that including a \"second step of distillation\" produced a student that performed better than simply fine-tuning the distilled language model:\n",
    "\n",
    "> We also studied whether we could add another step of distillation during the adaptation phase by fine-tuning DistilBERT on SQuAD using a BERT model previously fine-tuned on SQuAD as a teacher for an additional term in the loss (knowledge distillation). In this setting, there are thus two successive steps of distillation, one during the pre-training phase and one during the adaptation phase. In this case, we were able to reach interesting performances given the size of the model: 79.8 F1 and 70.4 EM, i.e. within 3 points of the full model.\n",
    "\n",
    "We'll take the same approach here and aim to reproduce the SQuAD v1 results from the paper. The results are summarised in the table below, where each entry refers to the Exact Match / F1-score on the validation set:\n",
    "\n",
    "| Implementation | BERT-base | DistilBERT | (DistilBERT)^2 |\n",
    "| :--- | :---: | :---: | :---: |\n",
    "| HuggingFace | 81.2 / 88.5 | 77.7 / 85.8 | 79.1 / 86.9 |\n",
    "| Ours | 80.1 / 87.8 | 76.7 / 85.2 | 78.4 / 86.5 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using transformers v4.1.1 and datasets v1.2.0\n",
      "Running on device: cuda\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd\n",
    "import datasets\n",
    "import transformers\n",
    "datasets.logging.set_verbosity_error()\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import (AutoTokenizer, AutoModelForQuestionAnswering, \n",
    "                          default_data_collator, QuestionAnsweringPipeline)\n",
    "\n",
    "from transformerlab.question_answering import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using transformers v{transformers.__version__} and datasets v{datasets.__version__}\")\n",
    "print(f\"Running on device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_ds = load_dataset(\"squad\")\n",
    "squad_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key information contained in each example is the `context`, `question` and `answers` fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answers': {'answer_start': [515], 'text': ['Saint Bernadette Soubirous']},\n",
      " 'context': 'Architecturally, the school has a Catholic character. Atop the '\n",
      "            \"Main Building's gold dome is a golden statue of the Virgin Mary. \"\n",
      "            'Immediately in front of the Main Building and facing it, is a '\n",
      "            'copper statue of Christ with arms upraised with the legend '\n",
      "            '\"Venite Ad Me Omnes\". Next to the Main Building is the Basilica '\n",
      "            'of the Sacred Heart. Immediately behind the basilica is the '\n",
      "            'Grotto, a Marian place of prayer and reflection. It is a replica '\n",
      "            'of the grotto at Lourdes, France where the Virgin Mary reputedly '\n",
      "            'appeared to Saint Bernadette Soubirous in 1858. At the end of the '\n",
      "            'main drive (and in a direct line that connects through 3 statues '\n",
      "            'and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
      " 'id': '5733be284776f41900661182',\n",
      " 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes '\n",
      "             'France?',\n",
      " 'title': 'University_of_Notre_Dame'}\n"
     ]
    }
   ],
   "source": [
    "pprint(squad_ds['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there is only one possible answer per examples in the training set (i.e. `answers.answer_start` has one element), but multiple possible answers in the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answers': {'answer_start': [69, 69, 73],\n",
      "             'text': ['the national anthem',\n",
      "                      'the national anthem',\n",
      "                      'national anthem']},\n",
      " 'context': 'Six-time Grammy winner and Academy Award nominee Lady Gaga '\n",
      "            'performed the national anthem, while Academy Award winner Marlee '\n",
      "            'Matlin provided American Sign Language (ASL) translation.',\n",
      " 'id': '56bec6ac3aeaaa14008c9400',\n",
      " 'question': 'What did Marlee Matlin translate?',\n",
      " 'title': 'Super_Bowl_50'}\n"
     ]
    }
   ],
   "source": [
    "pprint(squad_ds['validation'][666])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the frequencies by using a little bit of `Dataset.map` and `pandas` magic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    6238\n",
       "2    3498\n",
       "3     754\n",
       "4      71\n",
       "5       9\n",
       "Name: num_possible_answers, dtype: int64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers_ds = squad_ds.map(lambda x : {'num_possible_answers' : pd.Series(x['answers']['answer_start']).nunique()})\n",
    "answers_ds.set_format('pandas')\n",
    "answers_df = answers_ds['validation'][:]\n",
    "answers_df['num_possible_answers'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune BERT-base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do is fine-tune BERT-base so that we can use it as a teacher during the distillation step. To do so, let's tokenize and encode the texts using our helper functions from the `transformerlab.question_answering` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model_checkpoint = \"bert-base-uncased\"\n",
    "teacher_tokenizer = AutoTokenizer.from_pretrained(teacher_model_checkpoint)\n",
    "\n",
    "num_train_examples = len(squad_ds['train'])\n",
    "num_eval_examples = len(squad_ds['validation'])\n",
    "# note: convert_examples_to_features shuffles the train / eval datasets\n",
    "train_ds, eval_ds, eval_examples = convert_examples_to_features(squad_ds, teacher_tokenizer, num_train_examples, num_eval_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each encoded training examples contains the usual `input_ids`, `attention_mask` and `token_type_ids` associated with the BERT tokenizer, along with `start_positions` and `end_positions` to denote the span of text where the answer lies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what percentage of egyptians polled support death penalty for those leaving islam? the pew forum on religion & public life ranks egypt as the fifth worst country in the world for religious freedom. the united states commission on international religious freedom, a bipartisan independent agency of the us government, has placed egypt on its watch list of countries that require close monitoring due to the nature and extent of violations of religious freedom engaged in or tolerated by the government. according to a 2010 pew global attitudes survey, 84 % of egyptians polled supported the death penalty for those who leave islam ; 77 % supported whippings and cutting off of hands for theft and robbery ; and 82 % support stoning a person who commits adultery.'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_tokenizer.decode(train_ds[0]['input_ids'], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(97, 98)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0]['start_positions'], train_ds[0]['end_positions']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `input_ids` contain both the question _and_ context. As a sanity check, let's see that we can recover the original text by decoding the `input_ids` in one of the validation examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in what year did massachusetts first require children to be educated in schools? private schooling in the united states has been debated by educators, lawmakers and parents, since the beginnings of compulsory education in massachusetts in 1852. the supreme court precedent appears to favor educational choice, so long as states may set standards for educational accomplishment. some of the most relevant supreme court case law on this is as follows : runyon v. mccrary, 427 u. s. 160 ( 1976 ) ; wisconsin v. yoder, 406 u. s. 205 ( 1972 ) ; pierce v. society of sisters, 268 u. s. 510 ( 1925 ) ; meyer v. nebraska, 262 u. s. 390 ( 1923 ).'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_tokenizer.decode(eval_ds[0]['input_ids'], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answers': {'answer_start': [158, 158, 158], 'text': ['1852', '1852', '1852']},\n",
      " 'context': 'Private schooling in the United States has been debated by '\n",
      "            'educators, lawmakers and parents, since the beginnings of '\n",
      "            'compulsory education in Massachusetts in 1852. The Supreme Court '\n",
      "            'precedent appears to favor educational choice, so long as states '\n",
      "            'may set standards for educational accomplishment. Some of the '\n",
      "            'most relevant Supreme Court case law on this is as follows: '\n",
      "            'Runyon v. McCrary, 427 U.S. 160 (1976); Wisconsin v. Yoder, 406 '\n",
      "            'U.S. 205 (1972); Pierce v. Society of Sisters, 268 U.S. 510 '\n",
      "            '(1925); Meyer v. Nebraska, 262 U.S. 390 (1923).',\n",
      " 'id': '572759665951b619008f8884',\n",
      " 'question': 'In what year did Massachusetts first require children to be '\n",
      "             'educated in schools?',\n",
      " 'title': 'Private_school'}\n"
     ]
    }
   ],
   "source": [
    "pprint(eval_examples[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks good, so let's move on to fine-tuning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure and initialise the trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning Transformers for extractive question answering involves a significant amount of postprocessing to map the model's logits to spans of text for the predicted answers. Again we'll use the custom trainer from `transformerlab.question_answering` to do this for us. First we need to specify the training arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 88524\n",
      "Number of validation examples: 10784\n",
      "Number of raw validation examples: 10570\n",
      "Logging steps: 5532\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "logging_steps = len(train_ds) // batch_size\n",
    "\n",
    "teacher_args = QuestionAnsweringTrainingArguments(\n",
    "    output_dir=\"checkpoints\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=logging_steps)\n",
    "\n",
    "print(f\"Number of training examples: {train_ds.num_rows}\")\n",
    "print(f\"Number of validation examples: {eval_ds.num_rows}\")\n",
    "print(f\"Number of raw validation examples: {eval_examples.num_rows}\")\n",
    "print(f\"Logging steps: {logging_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we instantiate the trainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def teacher_init():\n",
    "    return AutoModelForQuestionAnswering.from_pretrained(teacher_model_checkpoint)\n",
    "\n",
    "teacher_trainer = QuestionAnsweringTrainer(\n",
    "    model_init=teacher_init,\n",
    "    args=teacher_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    eval_examples=eval_examples,\n",
    "    tokenizer=teacher_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and fine-tune (this takes around 2h on a single 16GB GPU):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_trainer.train();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we save the model so we can upload it to the HuggingFace model hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_trainer.save_model('models/bert-base-uncased-finetuned-squad-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate fine-tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've fine-tuned BERT-base on SQuAD, we can easily evaluate it by downloading from the Hub and initialising a new trainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='674' max='674' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [674/674 02:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f14506ffd8574937be690981f65dac0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10570.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 'No log',\n",
       " 'eval_exact_match': 80.07568590350047,\n",
       " 'eval_f1': 87.77870284880602}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_checkpoint = \"lewtun/bert-base-uncased-finetuned-squad-v1\"\n",
    "teacher_tokenizer = AutoTokenizer.from_pretrained(teacher_checkpoint)\n",
    "teacher_finetuned = AutoModelForQuestionAnswering.from_pretrained(teacher_checkpoint)\n",
    "\n",
    "teacher_trainer = QuestionAnsweringTrainer(\n",
    "    model=teacher_finetuned,\n",
    "    args=teacher_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    eval_examples=eval_examples,\n",
    "    tokenizer=teacher_tokenizer)\n",
    "\n",
    "teacher_trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These scores are within 1% of the values quoted in the DistilBERT paper, and probably due to slightly different choices of the hyperparameters. Let's move on to fine-tuning DistilBERT!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune DistilBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we follow the same steps as we did for BERT-base, beginning with tokenizing the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distilbert_checkpoint = \"distilbert-base-uncased\"\n",
    "distilbert_tokenizer = AutoTokenizer.from_pretrained(distilbert_checkpoint)\n",
    "\n",
    "num_train_examples = len(squad_ds['train'])\n",
    "num_eval_examples = len(squad_ds['validation'])\n",
    "train_ds, eval_ds, eval_examples = convert_examples_to_features(squad_ds, distilbert_tokenizer, num_train_examples, num_eval_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we configure and initialise the trainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "logging_steps = len(train_ds) // batch_size\n",
    "\n",
    "distilbert_args = QuestionAnsweringTrainingArguments(\n",
    "    output_dir=\"checkpoints\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=logging_steps,\n",
    "    disable_tqdm=False\n",
    ")\n",
    "\n",
    "print(f\"Number of training examples: {train_ds.num_rows}\")\n",
    "print(f\"Number of validation examples: {eval_ds.num_rows}\")\n",
    "print(f\"Number of raw validation examples: {eval_examples.num_rows}\")\n",
    "print(f\"Logging steps: {logging_steps}\")\n",
    "\n",
    "def distilbert_init():\n",
    "    return AutoModelForQuestionAnswering.from_pretrained(distilbert_checkpoint)\n",
    "\n",
    "distilbert_trainer = QuestionAnsweringTrainer(\n",
    "    model_init=distilbert_init,\n",
    "    args=distilbert_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    eval_examples=eval_examples,\n",
    "    tokenizer=distilbert_tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning takes about 1.5h on a single 16GB GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distilbert_trainer.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distilbert_trainer.save_model('models/bert-base-uncased-finetuned-squad-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distilling DistilBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main thing we need to implement task-specific distillation is augment the standard cross-entropy loss with a distillation term (see above equation). We can implement this by overriding the `compute_loss` method of the `QuestionAnsweringTrainer`, but first let's define the training arguments we'll need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistillationTrainingArguments(QuestionAnsweringTrainingArguments):\n",
    "    def __init__(self, *args, alpha_ce=0.5, alpha_distil=0.5, temperature=2.0, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        self.alpha_ce = alpha_ce\n",
    "        self.alpha_distil = alpha_distil\n",
    "        self.temperature = temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the trainer, we'll need a few ingredients:\n",
    "\n",
    "* We need two models (a teacher and student), and since the `model` attribute is the one that is optimized, we'll just add an attribute for the teacher\n",
    "* When we pass the question and context to the student or teacher, we get a range of scores (logits) for the start and end positions. Since we want to minimize the distance between the teacher and student predictions , we'll use the KL-divergence as our distillation loss\n",
    "* Once the distillation loss is computed, we take a linear combination with the cross-entropy to obtain our final loss function\n",
    "\n",
    "The following code does the trick:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistillationTrainer(QuestionAnsweringTrainer):\n",
    "    def __init__(self, *args, teacher_model=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.teacher = teacher_model\n",
    "        self.teacher.eval()\n",
    "        self.train_dataset.set_format(\n",
    "            type=self.train_dataset.format[\"type\"], columns=list(self.train_dataset.features.keys()))\n",
    "        \n",
    "    def compute_loss(self, model, inputs):\n",
    "        inputs_stu = {\n",
    "            \"input_ids\": inputs['input_ids'],\n",
    "            \"attention_mask\": inputs['attention_mask'],\n",
    "            \"start_positions\": inputs['start_positions'],\n",
    "            \"end_positions\": inputs['end_positions'],\n",
    "            }\n",
    "        outputs_stu = model(**inputs_stu)\n",
    "        loss = outputs_stu.loss\n",
    "        start_logits_stu = outputs_stu.start_logits\n",
    "        end_logits_stu = outputs_stu.end_logits\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs_tea = self.teacher(\n",
    "                input_ids=inputs[\"input_ids\"], \n",
    "                token_type_ids=inputs[\"token_type_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"])\n",
    "            start_logits_tea = outputs_tea.start_logits\n",
    "            end_logits_tea = outputs_tea.end_logits\n",
    "        assert start_logits_tea.size() == start_logits_stu.size()\n",
    "        assert end_logits_tea.size() == end_logits_stu.size()\n",
    "        \n",
    "        loss_fct = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "        loss_start = (loss_fct(\n",
    "            F.log_softmax(start_logits_stu / self.args.temperature, dim=-1),\n",
    "            F.softmax(start_logits_tea / self.args.temperature, dim=-1)) * (self.args.temperature ** 2))\n",
    "        loss_end = (loss_fct(\n",
    "            F.log_softmax(end_logits_stu / self.args.temperature, dim=-1),\n",
    "            F.softmax(end_logits_tea / self.args.temperature, dim=-1)) * (self.args.temperature ** 2))\n",
    "        loss_logits = (loss_start + loss_end) / 2.0\n",
    "        loss = self.args.alpha_distil * loss_logits + self.args.alpha_distil * loss\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's then a similar process to configure and initialise the trainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 88524\n",
      "Number of validation examples: 10784\n",
      "Number of raw validation examples: 10570\n",
      "Logging steps: 5532\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "logging_steps = len(train_ds) // batch_size\n",
    "\n",
    "student_training_args = DistillationTrainingArguments(\n",
    "    output_dir=f\"checkpoints\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=logging_steps,\n",
    ")\n",
    "\n",
    "print(f\"Number of training examples: {train_ds.num_rows}\")\n",
    "print(f\"Number of validation examples: {eval_ds.num_rows}\")\n",
    "print(f\"Number of raw validation examples: {eval_examples.num_rows}\")\n",
    "print(f\"Logging steps: {logging_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_checkpoint = \"lewtun/bert-base-uncased-finetuned-squad-v1\"\n",
    "student_checkpoint = \"distilbert-base-uncased\"\n",
    "teacher_model = AutoModelForQuestionAnswering.from_pretrained(teacher_checkpoint).to(device)\n",
    "student_model = AutoModelForQuestionAnswering.from_pretrained(student_checkpoint).to(device)\n",
    "student_tokenizer = AutoTokenizer.from_pretrained(student_checkpoint)\n",
    "\n",
    "distil_trainer = DistillationTrainer(\n",
    "    model=student_model,\n",
    "    teacher_model=teacher_model,\n",
    "    args=student_training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    eval_examples=eval_examples,\n",
    "    tokenizer=student_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='16599' max='16599' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16599/16599 2:27:26, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Exact Match</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.559650</td>\n",
       "      <td>No log</td>\n",
       "      <td>76.026490</td>\n",
       "      <td>84.654177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.783048</td>\n",
       "      <td>No log</td>\n",
       "      <td>77.861873</td>\n",
       "      <td>85.693001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.619299</td>\n",
       "      <td>No log</td>\n",
       "      <td>78.344371</td>\n",
       "      <td>86.211550</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1283be7d6b542dba92ba3928f26c3a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10570.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2787275fdcb4c458dd1287795282331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10570.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c29e9c8a1bd044f3a7fb93bff8d8b19d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10570.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=16599, training_loss=0.9873095414488996)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distil_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distil_trainer.save_model('models/distilbert-base-uncased-distilled-squad-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speed test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a simple benchmark, here we compare the time it takes for our teacher and student to generate 1,000 predictions on a CPU (to simulate a production environment). First, we load our fine-tuned models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_model_ckpt = 'lewtun/distilbert-base-uncased-distilled-squad-v1'\n",
    "teacher_model_ckpt = 'lewtun/bert-base-uncased-finetuned-squad-v1'\n",
    "\n",
    "student_tokenizer = AutoTokenizer.from_pretrained(student_model_ckpt)\n",
    "student_model = AutoModelForQuestionAnswering.from_pretrained(student_model_ckpt).to('cpu')\n",
    "\n",
    "teacher_tokenizer = AutoTokenizer.from_pretrained(teacher_model_ckpt)\n",
    "teacher_model = AutoModelForQuestionAnswering.from_pretrained(teacher_model_ckpt).to('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create two pipelines for the student and teacher:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_pipe = QuestionAnsweringPipeline(student_model, student_tokenizer)\n",
    "teacher_pipe = QuestionAnsweringPipeline(teacher_model, teacher_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then run the inference test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39min 55s, sys: 34.8 s, total: 40min 29s\n",
      "Wall time: 6min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for idx in range(1000):\n",
    "    context = squad_ds['validation'][idx]['context']\n",
    "    question = squad_ds['validation'][idx]['question']\n",
    "    teacher_pipe(question=question, context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19min 47s, sys: 13.3 s, total: 20min 1s\n",
      "Wall time: 3min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for idx in range(1000):\n",
    "    context = squad_ds['validation'][idx]['context']\n",
    "    question = squad_ds['validation'][idx]['question']\n",
    "    student_pipe(question=question, context=context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this example, we see roughly a 2x speedup from using a distilled model with less than 3% drop in Exact Match / F1-score!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
