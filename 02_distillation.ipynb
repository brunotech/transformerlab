{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Distillation\n",
    "> Classes for performing task-specific distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classes in this module are adapted from the following resources:\n",
    "\n",
    "* The scripts from the [distillation example](https://github.com/huggingface/transformers/tree/master/examples/research_projects/distillation) in the `transformers` repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#hide\n",
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformerlab.question_answering import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class DistillationTrainingArguments(QuestionAnsweringTrainingArguments):\n",
    "    def __init__(self, *args, alpha_ce=0.5, alpha_distil=0.5, temperature=2.0, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        self.alpha_ce = alpha_ce\n",
    "        self.alpha_distil = alpha_distil\n",
    "        self.temperature = temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class DistillationTrainer(QuestionAnsweringTrainer):\n",
    "    def __init__(self, *args, teacher_model=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.teacher = teacher_model\n",
    "        self.teacher.eval()\n",
    "        self.train_dataset.set_format(\n",
    "            type=self.train_dataset.format[\"type\"], columns=list(self.train_dataset.features.keys()))\n",
    "        \n",
    "    def compute_loss(self, model, inputs):\n",
    "        inputs_stu = {\n",
    "            \"input_ids\": inputs['input_ids'],\n",
    "            \"attention_mask\": inputs['attention_mask'],\n",
    "            \"start_positions\": inputs['start_positions'],\n",
    "            \"end_positions\": inputs['end_positions'],\n",
    "            }\n",
    "        outputs_stu = model(**inputs_stu)\n",
    "        loss = outputs_stu.loss\n",
    "        start_logits_stu = outputs_stu.start_logits\n",
    "        end_logits_stu = outputs_stu.end_logits\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs_tea = self.teacher(\n",
    "                input_ids=inputs[\"input_ids\"], \n",
    "                token_type_ids=inputs[\"token_type_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"])\n",
    "            start_logits_tea = outputs_tea.start_logits\n",
    "            end_logits_tea = outputs_tea.end_logits\n",
    "        assert start_logits_tea.size() == start_logits_stu.size()\n",
    "        assert end_logits_tea.size() == end_logits_stu.size()\n",
    "        \n",
    "        loss_fct = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "        loss_start = (loss_fct(\n",
    "            F.log_softmax(start_logits_stu / self.args.temperature, dim=-1),\n",
    "            F.softmax(start_logits_tea / self.args.temperature, dim=-1)) * (self.args.temperature ** 2))\n",
    "        loss_end = (loss_fct(\n",
    "            F.log_softmax(end_logits_stu / self.args.temperature, dim=-1),\n",
    "            F.softmax(end_logits_tea / self.args.temperature, dim=-1)) * (self.args.temperature ** 2))\n",
    "        loss_logits = (loss_start + loss_end) / 2.0\n",
    "        loss = self.args.alpha_distil * loss_logits + self.args.alpha_distil * loss\n",
    "        return loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
