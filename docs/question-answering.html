---

title: Question Answering


keywords: fastai
sidebar: home_sidebar

summary: "Functions and classes for question answering tasks"
description: "Functions and classes for question answering tasks"
nb_path: "01_question-answering.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: 01_question-answering.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Load-libraries">Load libraries<a class="anchor-link" href="#Load-libraries"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Dataset-preprocessing">Dataset preprocessing<a class="anchor-link" href="#Dataset-preprocessing"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="prepare_train_features" class="doc_header"><code>prepare_train_features</code><a href="https://github.com/lewtun/transformerlab/tree/master/transformerlab/question_answering.py#L14" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>prepare_train_features</code>(<strong><code>examples</code></strong>, <strong><code>tokenizer</code></strong>, <strong><code>pad_on_right</code></strong>, <strong><code>max_length</code></strong>, <strong><code>doc_stride</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="prepare_validation_features" class="doc_header"><code>prepare_validation_features</code><a href="https://github.com/lewtun/transformerlab/tree/master/transformerlab/question_answering.py#L87" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>prepare_validation_features</code>(<strong><code>examples</code></strong>, <strong><code>tokenizer</code></strong>, <strong><code>pad_on_right</code></strong>, <strong><code>max_length</code></strong>, <strong><code>doc_stride</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Trainer">Trainer<a class="anchor-link" href="#Trainer"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="QuestionAnsweringTrainingArguments" class="doc_header"><code>class</code> <code>QuestionAnsweringTrainingArguments</code><a href="https://github.com/lewtun/transformerlab/tree/master/transformerlab/question_answering.py#L129" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>QuestionAnsweringTrainingArguments</code>(<strong>*<code>args</code></strong>, <strong><code>max_length</code></strong>=<em><code>384</code></em>, <strong><code>doc_stride</code></strong>=<em><code>128</code></em>, <strong><code>version_2_with_negative</code></strong>=<em><code>False</code></em>, <strong><code>null_score_diff_threshold</code></strong>=<em><code>0.0</code></em>, <strong><code>n_best_size</code></strong>=<em><code>20</code></em>, <strong><code>max_answer_length</code></strong>=<em><code>30</code></em>, <strong>**<code>kwargs</code></strong>) :: <code>TrainingArguments</code></p>
</blockquote>
<p>TrainingArguments is the subset of the arguments we use in our example scripts <strong>which relate to the training loop
itself</strong>.</p>
<p>Using :class:<code>~transformers.HfArgumentParser</code> we can turn this class into <code>argparse
&lt;https://docs.python.org/3/library/argparse.html#module-argparse&gt;</code>__ arguments that can be specified on the command
line.</p>
<p>Parameters:
    output_dir (:obj:<code>str</code>):
        The output directory where the model predictions and checkpoints will be written.
    overwrite_output_dir (:obj:<code>bool</code>, <code>optional</code>, defaults to :obj:<code>False</code>):
        If :obj:<code>True</code>, overwrite the content of the output directory. Use this to continue training if
        :obj:<code>output_dir</code> points to a checkpoint directory.
    do_train (:obj:<code>bool</code>, <code>optional</code>, defaults to :obj:<code>False</code>):
        Whether to run training or not. This argument is not directly used by :class:<code>~transformers.Trainer</code>, it's
        intended to be used by your training/evaluation scripts instead. See the <code>example scripts
        &lt;https://github.com/huggingface/transformers/tree/master/examples&gt;</code><strong> for more details.
    do_eval (:obj:<code>bool</code>, <code>optional</code>):
        Whether to run evaluation on the validation set or not. Will be set to :obj:<code>True</code> if
        :obj:<code>evaluation_strategy</code> is different from :obj:<code>"no"</code>. This argument is not directly used by
        :class:<code>~transformers.Trainer</code>, it's intended to be used by your training/evaluation scripts instead. See
        the <code>example scripts &lt;https://github.com/huggingface/transformers/tree/master/examples&gt;</code></strong> for more
        details.
    do_predict (:obj:<code>bool</code>, <code>optional</code>, defaults to :obj:<code>False</code>):
        Whether to run predictions on the test set or not. This argument is not directly used by
        :class:<code>~transformers.Trainer</code>, it's intended to be used by your training/evaluation scripts instead. See
        the <code>example scripts &lt;https://github.com/huggingface/transformers/tree/master/examples&gt;</code>__ for more
        details.
    evaluation_strategy (:obj:<code>str</code> or :class:<code>~transformers.trainer_utils.EvaluationStrategy</code>, <code>optional</code>, defaults to :obj:<code>"no"</code>):
        The evaluation strategy to adopt during training. Possible values are:</p>

<pre><code>        * :obj:`"no"`: No evaluation is done during training.
        * :obj:`"steps"`: Evaluation is done (and logged) every :obj:`eval_steps`.
        * :obj:`"epoch"`: Evaluation is done at the end of each epoch.

prediction_loss_only (:obj:`bool`, `optional`, defaults to `False`):
    When performing evaluation and generating predictions, only returns the loss.
per_device_train_batch_size (:obj:`int`, `optional`, defaults to 8):
    The batch size per GPU/TPU core/CPU for training.
per_device_eval_batch_size (:obj:`int`, `optional`, defaults to 8):
    The batch size per GPU/TPU core/CPU for evaluation.
gradient_accumulation_steps (:obj:`int`, `optional`, defaults to 1):
    Number of updates steps to accumulate the gradients for, before performing a backward/update pass.

    .. warning::

        When using gradient accumulation, one step is counted as one step with backward pass. Therefore,
        logging, evaluation, save will be conducted every ``gradient_accumulation_steps * xxx_step`` training
        examples.
eval_accumulation_steps (:obj:`int`, `optional`):
    Number of predictions steps to accumulate the output tensors for, before moving the results to the CPU. If
    left unset, the whole predictions are accumulated on GPU/TPU before being moved to the CPU (faster but
    requires more memory).
learning_rate (:obj:`float`, `optional`, defaults to 5e-5):
    The initial learning rate for Adam.
weight_decay (:obj:`float`, `optional`, defaults to 0):
    The weight decay to apply (if not zero).
adam_beta1 (:obj:`float`, `optional`, defaults to 0.9):
    The beta1 hyperparameter for the Adam optimizer.
adam_beta2 (:obj:`float`, `optional`, defaults to 0.999):
    The beta2 hyperparameter for the Adam optimizer.
adam_epsilon (:obj:`float`, `optional`, defaults to 1e-8):
    The epsilon hyperparameter for the Adam optimizer.
max_grad_norm (:obj:`float`, `optional`, defaults to 1.0):
    Maximum gradient norm (for gradient clipping).
num_train_epochs(:obj:`float`, `optional`, defaults to 3.0):
    Total number of training epochs to perform (if not an integer, will perform the decimal part percents of
    the last epoch before stopping training).
max_steps (:obj:`int`, `optional`, defaults to -1):
    If set to a positive number, the total number of training steps to perform. Overrides
    :obj:`num_train_epochs`.
lr_scheduler_type (:obj:`str` or :class:`~transformers.SchedulerType`, `optional`, defaults to :obj:`"linear"`):
    The scheduler type to use. See the documentation of :class:`~transformers.SchedulerType` for all possible
    values.
warmup_steps (:obj:`int`, `optional`, defaults to 0):
    Number of steps used for a linear warmup from 0 to :obj:`learning_rate`.
logging_dir (:obj:`str`, `optional`):
    `TensorBoard &lt;https://www.tensorflow.org/tensorboard&gt;`__ log directory. Will default to
    `runs/**CURRENT_DATETIME_HOSTNAME**`.
logging_first_step (:obj:`bool`, `optional`, defaults to :obj:`False`):
    Whether to log and evaluate the first :obj:`global_step` or not.
logging_steps (:obj:`int`, `optional`, defaults to 500):
    Number of update steps between two logs.
save_steps (:obj:`int`, `optional`, defaults to 500):
    Number of updates steps before two checkpoint saves.
save_total_limit (:obj:`int`, `optional`):
    If a value is passed, will limit the total amount of checkpoints. Deletes the older checkpoints in
    :obj:`output_dir`.
no_cuda (:obj:`bool`, `optional`, defaults to :obj:`False`):
    Whether to not use CUDA even when it is available or not.
seed (:obj:`int`, `optional`, defaults to 42):
    Random seed for initialization.
fp16 (:obj:`bool`, `optional`, defaults to :obj:`False`):
    Whether to use 16-bit (mixed) precision training (through NVIDIA Apex) instead of 32-bit training.
fp16_opt_level (:obj:`str`, `optional`, defaults to 'O1'):
    For :obj:`fp16` training, Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']. See details
    on the `Apex documentation &lt;https://nvidia.github.io/apex/amp.html&gt;`__.
fp16_backend (:obj:`str`, `optional`, defaults to :obj:`"auto"`):
    The backend to use for mixed precision training. Must be one of :obj:`"auto"`, :obj:`"amp"` or
    :obj:`"apex"`. :obj:`"auto"` will use AMP or APEX depending on the PyTorch version detected, while the
    other choices will force the requested backend.
local_rank (:obj:`int`, `optional`, defaults to -1):
    Rank of the process during distributed training.
tpu_num_cores (:obj:`int`, `optional`):
    When training on TPU, the number of TPU cores (automatically passed by launcher script).
debug (:obj:`bool`, `optional`, defaults to :obj:`False`):
    When training on TPU, whether to print debug metrics or not.
dataloader_drop_last (:obj:`bool`, `optional`, defaults to :obj:`False`):
    Whether to drop the last incomplete batch (if the length of the dataset is not divisible by the batch size)
    or not.
eval_steps (:obj:`int`, `optional`):
    Number of update steps between two evaluations if :obj:`evaluation_strategy="steps"`. Will default to the
    same value as :obj:`logging_steps` if not set.
dataloader_num_workers (:obj:`int`, `optional`, defaults to 0):
    Number of subprocesses to use for data loading (PyTorch only). 0 means that the data will be loaded in the
    main process.
past_index (:obj:`int`, `optional`, defaults to -1):
    Some models like :doc:`TransformerXL &lt;../model_doc/transformerxl&gt;` or :doc`XLNet &lt;../model_doc/xlnet&gt;` can
    make use of the past hidden states for their predictions. If this argument is set to a positive int, the
    ``Trainer`` will use the corresponding output (usually index 2) as the past state and feed it to the model
    at the next training step under the keyword argument ``mems``.
run_name (:obj:`str`, `optional`):
    A descriptor for the run. Typically used for `wandb &lt;https://www.wandb.com/&gt;`_ logging.
disable_tqdm (:obj:`bool`, `optional`):
    Whether or not to disable the tqdm progress bars and table of metrics produced by
    :class:`~transformers.notebook.NotebookTrainingTracker` in Jupyter Notebooks. Will default to :obj:`True`
    if the logging level is set to warn or lower (default), :obj:`False` otherwise.
remove_unused_columns (:obj:`bool`, `optional`, defaults to :obj:`True`):
    If using :obj:`datasets.Dataset` datasets, whether or not to automatically remove the columns unused by the
    model forward method.

    (Note that this behavior is not implemented for :class:`~transformers.TFTrainer` yet.)
label_names (:obj:`List[str]`, `optional`):
    The list of keys in your dictionary of inputs that correspond to the labels.

    Will eventually default to :obj:`["labels"]` except if the model used is one of the
    :obj:`XxxForQuestionAnswering` in which case it will default to :obj:`["start_positions",
    "end_positions"]`.
load_best_model_at_end (:obj:`bool`, `optional`, defaults to :obj:`False`):
    Whether or not to load the best model found during training at the end of training.

    .. note::

        When set to :obj:`True`, the parameters :obj:`save_steps` will be ignored and the model will be saved
        after each evaluation.
metric_for_best_model (:obj:`str`, `optional`):
    Use in conjunction with :obj:`load_best_model_at_end` to specify the metric to use to compare two different
    models. Must be the name of a metric returned by the evaluation with or without the prefix :obj:`"eval_"`.
    Will default to :obj:`"loss"` if unspecified and :obj:`load_best_model_at_end=True` (to use the evaluation
    loss).

    If you set this value, :obj:`greater_is_better` will default to :obj:`True`. Don't forget to set it to
    :obj:`False` if your metric is better when lower.
greater_is_better (:obj:`bool`, `optional`):
    Use in conjunction with :obj:`load_best_model_at_end` and :obj:`metric_for_best_model` to specify if better
    models should have a greater metric or not. Will default to:

    - :obj:`True` if :obj:`metric_for_best_model` is set to a value that isn't :obj:`"loss"` or
      :obj:`"eval_loss"`.
    - :obj:`False` if :obj:`metric_for_best_model` is not set, or set to :obj:`"loss"` or :obj:`"eval_loss"`.
ignore_skip_data (:obj:`bool`, `optional`, defaults to :obj:`False`):
    When resuming training, whether or not to skip the epochs and batches to get the data loading at the same
    stage as in the previous training. If set to :obj:`True`, the training will begin faster (as that skipping
    step can take a long time) but will not yield the same results as the interrupted training would have.
sharded_ddp (:obj:`bool`, `optional`, defaults to :obj:`False`):
    Use Sharded DDP training from `FairScale &lt;https://github.com/facebookresearch/fairscale&gt;`__ (in distributed
    training only). This is an experimental feature.
deepspeed (:obj:`str`, `optional`):
    Use `Deepspeed &lt;https://github.com/microsoft/deepspeed&gt;`__. This is an experimental feature and its API may
    evolve in the future. The value is the location of its json config file (usually ``ds_config.json``).
label_smoothing_factor (:obj:`float`, `optional`, defaults to 0.0):
    The label smoothing factor to use. Zero means no label smoothing, otherwise the underlying onehot-encoded
    labels are changed from 0s and 1s to :obj:`label_smoothing_factor/num_labels` and :obj:`1 -
    label_smoothing_factor + label_smoothing_factor/num_labels` respectively.
adafactor (:obj:`bool`, `optional`, defaults to :obj:`False`):
    Whether or not to use the :class:`~transformers.Adafactor` optimizer instead of
    :class:`~transformers.AdamW`.
group_by_length (:obj:`bool`, `optional`, defaults to :obj:`False`):
    Whether or not to group together samples of roughly the same legnth in the training dataset (to minimize
    padding applied and be more efficient). Only useful if applying dynamic padding.</code></pre>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="QuestionAnsweringTrainer" class="doc_header"><code>class</code> <code>QuestionAnsweringTrainer</code><a href="https://github.com/lewtun/transformerlab/tree/master/transformerlab/question_answering.py#L142" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>QuestionAnsweringTrainer</code>(<strong>*<code>args</code></strong>, <strong><code>eval_examples</code></strong>=<em><code>None</code></em>, <strong>**<code>kwargs</code></strong>) :: <code>Trainer</code></p>
</blockquote>
<p>Trainer is a simple but feature-complete training and eval loop for PyTorch, optimized for 🤗 Transformers.</p>
<p>Args:
    model (:class:<code>~transformers.PreTrainedModel</code> or :obj:<code>torch.nn.Module</code>, <code>optional</code>):
        The model to train, evaluate or use for predictions. If not provided, a <code>model_init</code> must be passed.</p>

<pre><code>    .. note::

        :class:`~transformers.Trainer` is optimized to work with the :class:`~transformers.PreTrainedModel`
        provided by the library. You can still use your own models defined as :obj:`torch.nn.Module` as long as
        they work the same way as the 🤗 Transformers models.
args (:class:`~transformers.TrainingArguments`, `optional`):
    The arguments to tweak for training. Will default to a basic instance of
    :class:`~transformers.TrainingArguments` with the ``output_dir`` set to a directory named `tmp_trainer` in
    the current directory if not provided.
data_collator (:obj:`DataCollator`, `optional`):
    The function to use to form a batch from a list of elements of :obj:`train_dataset` or :obj:`eval_dataset`.
    Will default to :func:`~transformers.default_data_collator` if no ``tokenizer`` is provided, an instance of
    :func:`~transformers.DataCollatorWithPadding` otherwise.
train_dataset (:obj:`torch.utils.data.dataset.Dataset`, `optional`):
    The dataset to use for training. If it is an :obj:`datasets.Dataset`, columns not accepted by the
    ``model.forward()`` method are automatically removed.
eval_dataset (:obj:`torch.utils.data.dataset.Dataset`, `optional`):
     The dataset to use for evaluation. If it is an :obj:`datasets.Dataset`, columns not accepted by the
     ``model.forward()`` method are automatically removed.
tokenizer (:class:`PreTrainedTokenizerBase`, `optional`):
    The tokenizer used to preprocess the data. If provided, will be used to automatically pad the inputs the
    maximum length when batching inputs, and it will be saved along the model to make it easier to rerun an
    interrupted training or reuse the fine-tuned model.
model_init (:obj:`Callable[[], PreTrainedModel]`, `optional`):
    A function that instantiates the model to be used. If provided, each call to
    :meth:`~transformers.Trainer.train` will start from a new instance of the model as given by this function.

    The function may have zero argument, or a single one containing the optuna/Ray Tune trial object, to be
    able to choose different architectures according to hyper parameters (such as layer count, sizes of inner
    layers, dropout probabilities etc).
compute_metrics (:obj:`Callable[[EvalPrediction], Dict]`, `optional`):
    The function that will be used to compute metrics at evaluation. Must take a
    :class:`~transformers.EvalPrediction` and return a dictionary string to metric values.
callbacks (List of :obj:`~transformers.TrainerCallback`, `optional`):
    A list of callbacks to customize the training loop. Will add those to the list of default callbacks
    detailed in :doc:`here &lt;callback&gt;`.

    If you want to remove one of the default callbacks used, use the :meth:`Trainer.remove_callback` method.
optimizers (:obj:`Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR`, `optional`): A tuple
    containing the optimizer and the scheduler to use. Will default to an instance of
    :class:`~transformers.AdamW` on your model and a scheduler given by
    :func:`~transformers.get_linear_schedule_with_warmup` controlled by :obj:`args`.

</code></pre>
<p>Important attributes:</p>

<pre><code>- **model** -- Always points to the core model. If using a transformers model, it will be a
  :class:`~transformers.PreTrainedModel` subclass.
- **model_wrapped** -- Always points to the most external model in case one or more other modules wrap the
  original model. This is the model that should be used for the forward pass. For example, under ``DeepSpeed``,
  the inner model is wrapped in ``DeepSpeed`` and then again in ``torch.nn.DistributedDataParallel``. If the
  inner model hasn't been wrapped, then ``self.model_wrapped`` is the same as ``self.model``.
- **is_model_parallel** -- Whether or not a model has been switched to a model parallel mode (different from
  data parallelism, this means some of the model layers are split on different GPUs).</code></pre>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

</div>
 

