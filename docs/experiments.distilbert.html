---

title: DistilBERT


keywords: fastai
sidebar: home_sidebar

summary: "A partial reimplementation of DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter by Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf <a href='https://arxiv.org/abs/1910.01108'>[arXiv:1910.01108</a>]"
description: "A partial reimplementation of DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter by Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf <a href='https://arxiv.org/abs/1910.01108'>[arXiv:1910.01108</a>]"
nb_path: "experiments.distilbert.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: experiments.distilbert.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The goal of this notebook is to explore <em>task-specific</em> knowledge distillation, where a teacher is used to augment the cross-entropy loss of the student during fine-tuning:</p>
<p>{% raw %}
$${\cal L}(\mathbf{x}|T) = - \sum_i \bar{y}_i\log y_i(\mathbf{x}|T) -T^2 \sum_i \hat{y}_i(\mathbf{x}|T)\log y_i(\mathbf{x}|T) \,.$$
{% endraw %}</p>
<p>Here $T$ is the temperature, $\hat{y}$ are the outputs from the model, $\bar{y}$ the ground-truth labels, and $y_i$ a softmax with temperature.</p>
<p>This neat idea comes from the DistilBERT paper, where the authors found that including a "second step of distillation" produced a student that performed better than simply fine-tuning the distilled language model:</p>
<blockquote><p>We also studied whether we could add another step of distillation during the adaptation phase by fine-tuning DistilBERT on SQuAD using a BERT model previously fine-tuned on SQuAD as a teacher for an additional term in the loss (knowledge distillation). In this setting, there are thus two successive steps of distillation, one during the pre-training phase and one during the adaptation phase. In this case, we were able to reach interesting performances given the size of the model:79.8 F1 and 70.4 EM, i.e. within 3 points of the full model.
We'll take the same approach here and aim to reproduce the SQuAD v1 results from the paper. The results are summarised in the table below, where each entry refers to the Exact Match / F1-score on the validation set:</p>
</blockquote>
<table>
<thead><tr>
<th style="text-align:left">Implementation</th>
<th style="text-align:center">BERT-base</th>
<th style="text-align:center">DistilBERT</th>
<th style="text-align:center">(DistilBERT)^2</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">HuggingFace</td>
<td style="text-align:center">81.2 / 88.5</td>
<td style="text-align:center">77.7 / 85.8</td>
<td style="text-align:center">79.1 / 86.9</td>
</tr>
<tr>
<td style="text-align:left">Ours</td>
<td style="text-align:center">80.1 / 87.8</td>
<td style="text-align:center">76.7 / 85.2</td>
<td style="text-align:center">78.4 / 86.5</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Load-libraries">Load libraries<a class="anchor-link" href="#Load-libraries"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%</span><span class="k">load_ext</span> autoreload
<span class="o">%</span><span class="k">autoreload</span> 2
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">math</span>
<span class="kn">from</span> <span class="nn">pprint</span> <span class="kn">import</span> <span class="n">pprint</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>

<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">datasets</span>
<span class="kn">import</span> <span class="nn">transformers</span>
<span class="n">datasets</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">set_verbosity_error</span><span class="p">()</span>
<span class="n">transformers</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">set_verbosity_error</span><span class="p">()</span>

<span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span><span class="p">,</span> <span class="n">load_metric</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="p">(</span><span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForQuestionAnswering</span><span class="p">,</span> 
                          <span class="n">default_data_collator</span><span class="p">,</span> <span class="n">QuestionAnsweringPipeline</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">transformerlab.question_answering</span> <span class="kn">import</span> <span class="o">*</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Using transformers v</span><span class="si">{</span><span class="n">transformers</span><span class="o">.</span><span class="n">__version__</span><span class="si">}</span><span class="s2"> and datasets v</span><span class="si">{</span><span class="n">datasets</span><span class="o">.</span><span class="n">__version__</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Running on device: </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Using transformers v4.1.1 and datasets v1.2.0
Running on device: cuda
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Load-and-inspect-data">Load and inspect data<a class="anchor-link" href="#Load-and-inspect-data"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">squad_ds</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;squad&quot;</span><span class="p">)</span>
<span class="n">squad_ds</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>DatasetDict({
    train: Dataset({
        features: [&#39;id&#39;, &#39;title&#39;, &#39;context&#39;, &#39;question&#39;, &#39;answers&#39;],
        num_rows: 87599
    })
    validation: Dataset({
        features: [&#39;id&#39;, &#39;title&#39;, &#39;context&#39;, &#39;question&#39;, &#39;answers&#39;],
        num_rows: 10570
    })
})</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The key information contained in each example is the <code>context</code>, <code>question</code> and <code>answers</code> fields:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">pprint</span><span class="p">(</span><span class="n">squad_ds</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>{&#39;answers&#39;: {&#39;answer_start&#39;: [515], &#39;text&#39;: [&#39;Saint Bernadette Soubirous&#39;]},
 &#39;context&#39;: &#39;Architecturally, the school has a Catholic character. Atop the &#39;
            &#34;Main Building&#39;s gold dome is a golden statue of the Virgin Mary. &#34;
            &#39;Immediately in front of the Main Building and facing it, is a &#39;
            &#39;copper statue of Christ with arms upraised with the legend &#39;
            &#39;&#34;Venite Ad Me Omnes&#34;. Next to the Main Building is the Basilica &#39;
            &#39;of the Sacred Heart. Immediately behind the basilica is the &#39;
            &#39;Grotto, a Marian place of prayer and reflection. It is a replica &#39;
            &#39;of the grotto at Lourdes, France where the Virgin Mary reputedly &#39;
            &#39;appeared to Saint Bernadette Soubirous in 1858. At the end of the &#39;
            &#39;main drive (and in a direct line that connects through 3 statues &#39;
            &#39;and the Gold Dome), is a simple, modern stone statue of Mary.&#39;,
 &#39;id&#39;: &#39;5733be284776f41900661182&#39;,
 &#39;question&#39;: &#39;To whom did the Virgin Mary allegedly appear in 1858 in Lourdes &#39;
             &#39;France?&#39;,
 &#39;title&#39;: &#39;University_of_Notre_Dame&#39;}
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Note that there is only one possible answer per examples in the training set (i.e. <code>answers.answer_start</code> has one element), but multiple possible answers in the validation set:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">pprint</span><span class="p">(</span><span class="n">squad_ds</span><span class="p">[</span><span class="s1">&#39;validation&#39;</span><span class="p">][</span><span class="mi">666</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>{&#39;answers&#39;: {&#39;answer_start&#39;: [69, 69, 73],
             &#39;text&#39;: [&#39;the national anthem&#39;,
                      &#39;the national anthem&#39;,
                      &#39;national anthem&#39;]},
 &#39;context&#39;: &#39;Six-time Grammy winner and Academy Award nominee Lady Gaga &#39;
            &#39;performed the national anthem, while Academy Award winner Marlee &#39;
            &#39;Matlin provided American Sign Language (ASL) translation.&#39;,
 &#39;id&#39;: &#39;56bec6ac3aeaaa14008c9400&#39;,
 &#39;question&#39;: &#39;What did Marlee Matlin translate?&#39;,
 &#39;title&#39;: &#39;Super_Bowl_50&#39;}
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can look at the frequencies by using a little bit of <code>Dataset.map</code> and <code>pandas</code> magic:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">answers_ds</span> <span class="o">=</span> <span class="n">squad_ds</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="p">{</span><span class="s1">&#39;num_possible_answers&#39;</span> <span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="s1">&#39;answers&#39;</span><span class="p">][</span><span class="s1">&#39;answer_start&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">nunique</span><span class="p">()})</span>
<span class="n">answers_ds</span><span class="o">.</span><span class="n">set_format</span><span class="p">(</span><span class="s1">&#39;pandas&#39;</span><span class="p">)</span>
<span class="n">answers_df</span> <span class="o">=</span> <span class="n">answers_ds</span><span class="p">[</span><span class="s1">&#39;validation&#39;</span><span class="p">][:]</span>
<span class="n">answers_df</span><span class="p">[</span><span class="s1">&#39;num_possible_answers&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>1    6238
2    3498
3     754
4      71
5       9
Name: num_possible_answers, dtype: int64</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Fine-tune-BERT-base">Fine-tune BERT-base<a class="anchor-link" href="#Fine-tune-BERT-base"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The first thing we need to do is fine-tune BERT-base so that we can use it as a teacher during the distillation step. To do so, let's tokenize and encode the texts using our helper functions from the <code>transformerlab.question_answering</code> module:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">teacher_model_checkpoint</span> <span class="o">=</span> <span class="s2">&quot;bert-base-uncased&quot;</span>
<span class="n">teacher_tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">teacher_model_checkpoint</span><span class="p">)</span>

<span class="n">num_train_examples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">squad_ds</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">])</span>
<span class="n">num_eval_examples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">squad_ds</span><span class="p">[</span><span class="s1">&#39;validation&#39;</span><span class="p">])</span>
<span class="c1"># note: convert_examples_to_features shuffles the train / eval datasets</span>
<span class="n">train_ds</span><span class="p">,</span> <span class="n">eval_ds</span><span class="p">,</span> <span class="n">eval_examples</span> <span class="o">=</span> <span class="n">convert_examples_to_features</span><span class="p">(</span><span class="n">squad_ds</span><span class="p">,</span> <span class="n">teacher_tokenizer</span><span class="p">,</span> <span class="n">num_train_examples</span><span class="p">,</span> <span class="n">num_eval_examples</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Each encoded training examples contains the usual <code>input_ids</code>, <code>attention_mask</code> and <code>token_type_ids</code> associated with the BERT tokenizer, along with <code>start_positions</code> and <code>end_positions</code> to denote the span of text where the answer lies:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">teacher_tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">train_ds</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;input_ids&#39;</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>&#39;what percentage of egyptians polled support death penalty for those leaving islam? the pew forum on religion &amp; public life ranks egypt as the fifth worst country in the world for religious freedom. the united states commission on international religious freedom, a bipartisan independent agency of the us government, has placed egypt on its watch list of countries that require close monitoring due to the nature and extent of violations of religious freedom engaged in or tolerated by the government. according to a 2010 pew global attitudes survey, 84 % of egyptians polled supported the death penalty for those who leave islam ; 77 % supported whippings and cutting off of hands for theft and robbery ; and 82 % support stoning a person who commits adultery.&#39;</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">train_ds</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;start_positions&#39;</span><span class="p">],</span> <span class="n">train_ds</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;end_positions&#39;</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(97, 98)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Note that the <code>input_ids</code> contain both the question <em>and</em> context. As a sanity check, let's see that we can recover the original text by decoding the <code>input_ids</code> in one of the validation examples:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">teacher_tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">eval_ds</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;input_ids&#39;</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>&#39;in what year did massachusetts first require children to be educated in schools? private schooling in the united states has been debated by educators, lawmakers and parents, since the beginnings of compulsory education in massachusetts in 1852. the supreme court precedent appears to favor educational choice, so long as states may set standards for educational accomplishment. some of the most relevant supreme court case law on this is as follows : runyon v. mccrary, 427 u. s. 160 ( 1976 ) ; wisconsin v. yoder, 406 u. s. 205 ( 1972 ) ; pierce v. society of sisters, 268 u. s. 510 ( 1925 ) ; meyer v. nebraska, 262 u. s. 390 ( 1923 ).&#39;</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">pprint</span><span class="p">(</span><span class="n">eval_examples</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>{&#39;answers&#39;: {&#39;answer_start&#39;: [158, 158, 158], &#39;text&#39;: [&#39;1852&#39;, &#39;1852&#39;, &#39;1852&#39;]},
 &#39;context&#39;: &#39;Private schooling in the United States has been debated by &#39;
            &#39;educators, lawmakers and parents, since the beginnings of &#39;
            &#39;compulsory education in Massachusetts in 1852. The Supreme Court &#39;
            &#39;precedent appears to favor educational choice, so long as states &#39;
            &#39;may set standards for educational accomplishment. Some of the &#39;
            &#39;most relevant Supreme Court case law on this is as follows: &#39;
            &#39;Runyon v. McCrary, 427 U.S. 160 (1976); Wisconsin v. Yoder, 406 &#39;
            &#39;U.S. 205 (1972); Pierce v. Society of Sisters, 268 U.S. 510 &#39;
            &#39;(1925); Meyer v. Nebraska, 262 U.S. 390 (1923).&#39;,
 &#39;id&#39;: &#39;572759665951b619008f8884&#39;,
 &#39;question&#39;: &#39;In what year did Massachusetts first require children to be &#39;
             &#39;educated in schools?&#39;,
 &#39;title&#39;: &#39;Private_school&#39;}
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This looks good, so let's move on to fine-tuning!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Configure-and-initialise-the-trainer">Configure and initialise the trainer<a class="anchor-link" href="#Configure-and-initialise-the-trainer"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Fine-tuning Transformers for extractive question answering involves a significant amount of postprocessing to map the model's logits to spans of text for the predicted answers. Again we'll use the custom trainer from <code>transformerlab.question_answering</code> to do this for us. First we need to specify the training arguments:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">logging_steps</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_ds</span><span class="p">)</span> <span class="o">//</span> <span class="n">batch_size</span>

<span class="n">teacher_args</span> <span class="o">=</span> <span class="n">QuestionAnsweringTrainingArguments</span><span class="p">(</span>
    <span class="n">output_dir</span><span class="o">=</span><span class="s2">&quot;checkpoints&quot;</span><span class="p">,</span>
    <span class="n">evaluation_strategy</span> <span class="o">=</span> <span class="s2">&quot;epoch&quot;</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">2e-5</span><span class="p">,</span>
    <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
    <span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
    <span class="n">num_train_epochs</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
    <span class="n">logging_steps</span><span class="o">=</span><span class="n">logging_steps</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of training examples: </span><span class="si">{</span><span class="n">train_ds</span><span class="o">.</span><span class="n">num_rows</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of validation examples: </span><span class="si">{</span><span class="n">eval_ds</span><span class="o">.</span><span class="n">num_rows</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of raw validation examples: </span><span class="si">{</span><span class="n">eval_examples</span><span class="o">.</span><span class="n">num_rows</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Logging steps: </span><span class="si">{</span><span class="n">logging_steps</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Number of training examples: 88524
Number of validation examples: 10784
Number of raw validation examples: 10570
Logging steps: 5532
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Next we instantiate the trainer:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">teacher_init</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">AutoModelForQuestionAnswering</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">teacher_model_checkpoint</span><span class="p">)</span>

<span class="n">teacher_trainer</span> <span class="o">=</span> <span class="n">QuestionAnsweringTrainer</span><span class="p">(</span>
    <span class="n">model_init</span><span class="o">=</span><span class="n">teacher_init</span><span class="p">,</span>
    <span class="n">args</span><span class="o">=</span><span class="n">teacher_args</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">train_ds</span><span class="p">,</span>
    <span class="n">eval_dataset</span><span class="o">=</span><span class="n">eval_ds</span><span class="p">,</span>
    <span class="n">eval_examples</span><span class="o">=</span><span class="n">eval_examples</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">teacher_tokenizer</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>and fine-tune (this takes around 2h on a single 16GB GPU):</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">teacher_trainer</span><span class="o">.</span><span class="n">train</span><span class="p">();</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Finally, we save the model so we can upload it to the HuggingFace model hub:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">teacher_trainer</span><span class="o">.</span><span class="n">save_model</span><span class="p">(</span><span class="s1">&#39;models/bert-base-uncased-finetuned-squad-v1&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Evaluate-fine-tuned-model">Evaluate fine-tuned model<a class="anchor-link" href="#Evaluate-fine-tuned-model"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now that we've fine-tuned BERT-base on SQuAD, we can easily evaluate it by downloading from the Hub and initialising a new trainer:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">teacher_checkpoint</span> <span class="o">=</span> <span class="s2">&quot;lewtun/bert-base-uncased-finetuned-squad-v1&quot;</span>
<span class="n">teacher_tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">teacher_checkpoint</span><span class="p">)</span>
<span class="n">teacher_finetuned</span> <span class="o">=</span> <span class="n">AutoModelForQuestionAnswering</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">teacher_checkpoint</span><span class="p">)</span>

<span class="n">teacher_trainer</span> <span class="o">=</span> <span class="n">QuestionAnsweringTrainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">teacher_finetuned</span><span class="p">,</span>
    <span class="n">args</span><span class="o">=</span><span class="n">teacher_args</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">train_ds</span><span class="p">,</span>
    <span class="n">eval_dataset</span><span class="o">=</span><span class="n">eval_ds</span><span class="p">,</span>
    <span class="n">eval_examples</span><span class="o">=</span><span class="n">eval_examples</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">teacher_tokenizer</span><span class="p">)</span>

<span class="n">teacher_trainer</span><span class="o">.</span><span class="n">evaluate</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

    <div>
        <style>
            /* Turns off some styling */
            progress {
                /* gets rid of default border in Firefox and Opera. */
                border: none;
                /* Needs to be in here for Safari polyfill so background images work as expected. */
                background-size: auto;
            }
        </style>
      
      <progress value='674' max='674' style='width:300px; height:20px; vertical-align: middle;'></progress>
      [674/674 02:09]
    </div>
    
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>
</pre>
</div>
</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>{&#39;eval_loss&#39;: &#39;No log&#39;,
 &#39;eval_exact_match&#39;: 80.07568590350047,
 &#39;eval_f1&#39;: 87.77870284880602}</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>These scores are within 1% of the values quoted in the DistilBERT paper, and probably due to slightly different choices of the hyperparameters. Let's move on to fine-tuning DistilBERT!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Fine-tune-DistilBERT">Fine-tune DistilBERT<a class="anchor-link" href="#Fine-tune-DistilBERT"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here we follow the same steps as we did for BERT-base, beginning with tokenizing the datasets:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">distilbert_checkpoint</span> <span class="o">=</span> <span class="s2">&quot;distilbert-base-uncased&quot;</span>
<span class="n">distilbert_tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">distilbert_checkpoint</span><span class="p">)</span>

<span class="n">num_train_examples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">squad_ds</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">])</span>
<span class="n">num_eval_examples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">squad_ds</span><span class="p">[</span><span class="s1">&#39;validation&#39;</span><span class="p">])</span>
<span class="n">train_ds</span><span class="p">,</span> <span class="n">eval_ds</span><span class="p">,</span> <span class="n">eval_examples</span> <span class="o">=</span> <span class="n">convert_examples_to_features</span><span class="p">(</span><span class="n">squad_ds</span><span class="p">,</span> <span class="n">distilbert_tokenizer</span><span class="p">,</span> <span class="n">num_train_examples</span><span class="p">,</span> <span class="n">num_eval_examples</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Next, we configure and initialise the trainer:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">logging_steps</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_ds</span><span class="p">)</span> <span class="o">//</span> <span class="n">batch_size</span>

<span class="n">distilbert_args</span> <span class="o">=</span> <span class="n">QuestionAnsweringTrainingArguments</span><span class="p">(</span>
    <span class="n">output_dir</span><span class="o">=</span><span class="s2">&quot;checkpoints&quot;</span><span class="p">,</span>
    <span class="n">evaluation_strategy</span> <span class="o">=</span> <span class="s2">&quot;epoch&quot;</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">2e-5</span><span class="p">,</span>
    <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
    <span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
    <span class="n">num_train_epochs</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
    <span class="n">logging_steps</span><span class="o">=</span><span class="n">logging_steps</span><span class="p">,</span>
    <span class="n">disable_tqdm</span><span class="o">=</span><span class="kc">False</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of training examples: </span><span class="si">{</span><span class="n">train_ds</span><span class="o">.</span><span class="n">num_rows</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of validation examples: </span><span class="si">{</span><span class="n">eval_ds</span><span class="o">.</span><span class="n">num_rows</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of raw validation examples: </span><span class="si">{</span><span class="n">eval_examples</span><span class="o">.</span><span class="n">num_rows</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Logging steps: </span><span class="si">{</span><span class="n">logging_steps</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">distilbert_init</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">AutoModelForQuestionAnswering</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">distilbert_checkpoint</span><span class="p">)</span>

<span class="n">distilbert_trainer</span> <span class="o">=</span> <span class="n">QuestionAnsweringTrainer</span><span class="p">(</span>
    <span class="n">model_init</span><span class="o">=</span><span class="n">distilbert_init</span><span class="p">,</span>
    <span class="n">args</span><span class="o">=</span><span class="n">distilbert_args</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">train_ds</span><span class="p">,</span>
    <span class="n">eval_dataset</span><span class="o">=</span><span class="n">eval_ds</span><span class="p">,</span>
    <span class="n">eval_examples</span><span class="o">=</span><span class="n">eval_examples</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">distilbert_tokenizer</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Fine-tuning takes about 1.5h on a single 16GB GPU:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">distilbert_trainer</span><span class="o">.</span><span class="n">train</span><span class="p">();</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">distilbert_trainer</span><span class="o">.</span><span class="n">save_model</span><span class="p">(</span><span class="s1">&#39;models/bert-base-uncased-finetuned-squad-v1&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Distilling-DistilBERT">Distilling DistilBERT<a class="anchor-link" href="#Distilling-DistilBERT"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The main thing we need to implement task-specific distillation is augment the standard cross-entropy loss with a distillation term (see above equation). We can implement this by overriding the <code>compute_loss</code> method of the <a href="/transformerlab/question-answering.html#QuestionAnsweringTrainer"><code>QuestionAnsweringTrainer</code></a>, but first let's define the training arguments we'll need:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">DistillationTrainingArguments</span><span class="p">(</span><span class="n">QuestionAnsweringTrainingArguments</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="n">alpha_ce</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">alpha_distil</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha_ce</span> <span class="o">=</span> <span class="n">alpha_ce</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha_distil</span> <span class="o">=</span> <span class="n">alpha_distil</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">temperature</span> <span class="o">=</span> <span class="n">temperature</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>For the trainer, we'll need a few ingredients:</p>
<ul>
<li>We need two models (a teacher and student), and since the <code>model</code> attribute is the one that is optimized, we'll just add an attribute for the teacher</li>
<li>When we pass the question and context to the student or teacher, we get a range of scores (logits) for the start and end positions. Since we want to minimize the distance between the teacher and student predictions , we'll use the KL-divergence as our distillation loss</li>
<li>Once the distillation loss is computed, we take a linear combination with the cross-entropy to obtain our final loss function</li>
</ul>
<p>The following code does the trick:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">DistillationTrainer</span><span class="p">(</span><span class="n">QuestionAnsweringTrainer</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="n">teacher_model</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">teacher</span> <span class="o">=</span> <span class="n">teacher_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">teacher</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_dataset</span><span class="o">.</span><span class="n">set_format</span><span class="p">(</span>
            <span class="nb">type</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">train_dataset</span><span class="o">.</span><span class="n">format</span><span class="p">[</span><span class="s2">&quot;type&quot;</span><span class="p">],</span> <span class="n">columns</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_dataset</span><span class="o">.</span><span class="n">features</span><span class="o">.</span><span class="n">keys</span><span class="p">()))</span>
        
    <span class="k">def</span> <span class="nf">compute_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">inputs_stu</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="n">inputs</span><span class="p">[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">],</span>
            <span class="s2">&quot;attention_mask&quot;</span><span class="p">:</span> <span class="n">inputs</span><span class="p">[</span><span class="s1">&#39;attention_mask&#39;</span><span class="p">],</span>
            <span class="s2">&quot;start_positions&quot;</span><span class="p">:</span> <span class="n">inputs</span><span class="p">[</span><span class="s1">&#39;start_positions&#39;</span><span class="p">],</span>
            <span class="s2">&quot;end_positions&quot;</span><span class="p">:</span> <span class="n">inputs</span><span class="p">[</span><span class="s1">&#39;end_positions&#39;</span><span class="p">],</span>
            <span class="p">}</span>
        <span class="n">outputs_stu</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs_stu</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">outputs_stu</span><span class="o">.</span><span class="n">loss</span>
        <span class="n">start_logits_stu</span> <span class="o">=</span> <span class="n">outputs_stu</span><span class="o">.</span><span class="n">start_logits</span>
        <span class="n">end_logits_stu</span> <span class="o">=</span> <span class="n">outputs_stu</span><span class="o">.</span><span class="n">end_logits</span>
        
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">outputs_tea</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">teacher</span><span class="p">(</span>
                <span class="n">input_ids</span><span class="o">=</span><span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">],</span> 
                <span class="n">token_type_ids</span><span class="o">=</span><span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;token_type_ids&quot;</span><span class="p">],</span>
                <span class="n">attention_mask</span><span class="o">=</span><span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">])</span>
            <span class="n">start_logits_tea</span> <span class="o">=</span> <span class="n">outputs_tea</span><span class="o">.</span><span class="n">start_logits</span>
            <span class="n">end_logits_tea</span> <span class="o">=</span> <span class="n">outputs_tea</span><span class="o">.</span><span class="n">end_logits</span>
        <span class="k">assert</span> <span class="n">start_logits_tea</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="n">start_logits_stu</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
        <span class="k">assert</span> <span class="n">end_logits_tea</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="n">end_logits_stu</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
        
        <span class="n">loss_fct</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">KLDivLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;batchmean&quot;</span><span class="p">)</span>
        <span class="n">loss_start</span> <span class="o">=</span> <span class="p">(</span><span class="n">loss_fct</span><span class="p">(</span>
            <span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">start_logits_stu</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">temperature</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">start_logits_tea</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">temperature</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">temperature</span> <span class="o">**</span> <span class="mi">2</span><span class="p">))</span>
        <span class="n">loss_end</span> <span class="o">=</span> <span class="p">(</span><span class="n">loss_fct</span><span class="p">(</span>
            <span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">end_logits_stu</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">temperature</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">end_logits_tea</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">temperature</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">temperature</span> <span class="o">**</span> <span class="mi">2</span><span class="p">))</span>
        <span class="n">loss_logits</span> <span class="o">=</span> <span class="p">(</span><span class="n">loss_start</span> <span class="o">+</span> <span class="n">loss_end</span><span class="p">)</span> <span class="o">/</span> <span class="mf">2.0</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">alpha_distil</span> <span class="o">*</span> <span class="n">loss_logits</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">alpha_distil</span> <span class="o">*</span> <span class="n">loss</span>
        <span class="k">return</span> <span class="n">loss</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It's then a similar process to configure and initialise the trainer:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">logging_steps</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_ds</span><span class="p">)</span> <span class="o">//</span> <span class="n">batch_size</span>

<span class="n">student_training_args</span> <span class="o">=</span> <span class="n">DistillationTrainingArguments</span><span class="p">(</span>
    <span class="n">output_dir</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;checkpoints&quot;</span><span class="p">,</span>
    <span class="n">evaluation_strategy</span> <span class="o">=</span> <span class="s2">&quot;epoch&quot;</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">2e-5</span><span class="p">,</span>
    <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
    <span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
    <span class="n">num_train_epochs</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
    <span class="n">logging_steps</span><span class="o">=</span><span class="n">logging_steps</span><span class="p">,</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of training examples: </span><span class="si">{</span><span class="n">train_ds</span><span class="o">.</span><span class="n">num_rows</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of validation examples: </span><span class="si">{</span><span class="n">eval_ds</span><span class="o">.</span><span class="n">num_rows</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of raw validation examples: </span><span class="si">{</span><span class="n">eval_examples</span><span class="o">.</span><span class="n">num_rows</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Logging steps: </span><span class="si">{</span><span class="n">logging_steps</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Number of training examples: 88524
Number of validation examples: 10784
Number of raw validation examples: 10570
Logging steps: 5532
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">teacher_checkpoint</span> <span class="o">=</span> <span class="s2">&quot;lewtun/bert-base-uncased-finetuned-squad-v1&quot;</span>
<span class="n">student_checkpoint</span> <span class="o">=</span> <span class="s2">&quot;distilbert-base-uncased&quot;</span>
<span class="n">teacher_model</span> <span class="o">=</span> <span class="n">AutoModelForQuestionAnswering</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">teacher_checkpoint</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">student_model</span> <span class="o">=</span> <span class="n">AutoModelForQuestionAnswering</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">student_checkpoint</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">student_tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">student_checkpoint</span><span class="p">)</span>

<span class="n">distil_trainer</span> <span class="o">=</span> <span class="n">DistillationTrainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">student_model</span><span class="p">,</span>
    <span class="n">teacher_model</span><span class="o">=</span><span class="n">teacher_model</span><span class="p">,</span>
    <span class="n">args</span><span class="o">=</span><span class="n">student_training_args</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">train_ds</span><span class="p">,</span>
    <span class="n">eval_dataset</span><span class="o">=</span><span class="n">eval_ds</span><span class="p">,</span>
    <span class="n">eval_examples</span><span class="o">=</span><span class="n">eval_examples</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">student_tokenizer</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">distil_trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

    <div>
        <style>
            /* Turns off some styling */
            progress {
                /* gets rid of default border in Firefox and Opera. */
                border: none;
                /* Needs to be in here for Safari polyfill so background images work as expected. */
                background-size: auto;
            }
        </style>
      
      <progress value='16599' max='16599' style='width:300px; height:20px; vertical-align: middle;'></progress>
      [16599/16599 2:27:26, Epoch 3/3]
    </div>
    <table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>Epoch</th>
      <th>Training Loss</th>
      <th>Validation Loss</th>
      <th>Exact Match</th>
      <th>F1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1.000000</td>
      <td>1.559650</td>
      <td>No log</td>
      <td>76.026490</td>
      <td>84.654177</td>
    </tr>
    <tr>
      <td>2.000000</td>
      <td>0.783048</td>
      <td>No log</td>
      <td>77.861873</td>
      <td>85.693001</td>
    </tr>
    <tr>
      <td>3.000000</td>
      <td>0.619299</td>
      <td>No log</td>
      <td>78.344371</td>
      <td>86.211550</td>
    </tr>
  </tbody>
</table><p>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>


</pre>
</div>
</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>TrainOutput(global_step=16599, training_loss=0.9873095414488996)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">distil_trainer</span><span class="o">.</span><span class="n">save_model</span><span class="p">(</span><span class="s1">&#39;models/distilbert-base-uncased-distilled-squad-v1&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Speed-test">Speed test<a class="anchor-link" href="#Speed-test"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As a simple benchmark, here we compare the time it takes for our teacher and student to generate 1,000 predictions on a CPU (to simulate a production environment). First, we load our fine-tuned models:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">student_model_ckpt</span> <span class="o">=</span> <span class="s1">&#39;lewtun/distilbert-base-uncased-distilled-squad-v1&#39;</span>
<span class="n">teacher_model_ckpt</span> <span class="o">=</span> <span class="s1">&#39;lewtun/bert-base-uncased-finetuned-squad-v1&#39;</span>

<span class="n">student_tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">student_model_ckpt</span><span class="p">)</span>
<span class="n">student_model</span> <span class="o">=</span> <span class="n">AutoModelForQuestionAnswering</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">student_model_ckpt</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span>

<span class="n">teacher_tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">teacher_model_ckpt</span><span class="p">)</span>
<span class="n">teacher_model</span> <span class="o">=</span> <span class="n">AutoModelForQuestionAnswering</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">teacher_model_ckpt</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Next we create two pipelines for the student and teacher:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">student_pipe</span> <span class="o">=</span> <span class="n">QuestionAnsweringPipeline</span><span class="p">(</span><span class="n">student_model</span><span class="p">,</span> <span class="n">student_tokenizer</span><span class="p">)</span>
<span class="n">teacher_pipe</span> <span class="o">=</span> <span class="n">QuestionAnsweringPipeline</span><span class="p">(</span><span class="n">teacher_model</span><span class="p">,</span> <span class="n">teacher_tokenizer</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And then run the inference test:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%%time</span>

<span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">context</span> <span class="o">=</span> <span class="n">squad_ds</span><span class="p">[</span><span class="s1">&#39;validation&#39;</span><span class="p">][</span><span class="n">idx</span><span class="p">][</span><span class="s1">&#39;context&#39;</span><span class="p">]</span>
    <span class="n">question</span> <span class="o">=</span> <span class="n">squad_ds</span><span class="p">[</span><span class="s1">&#39;validation&#39;</span><span class="p">][</span><span class="n">idx</span><span class="p">][</span><span class="s1">&#39;question&#39;</span><span class="p">]</span>
    <span class="n">teacher_pipe</span><span class="p">(</span><span class="n">question</span><span class="o">=</span><span class="n">question</span><span class="p">,</span> <span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>CPU times: user 39min 55s, sys: 34.8 s, total: 40min 29s
Wall time: 6min 7s
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%%time</span>

<span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">context</span> <span class="o">=</span> <span class="n">squad_ds</span><span class="p">[</span><span class="s1">&#39;validation&#39;</span><span class="p">][</span><span class="n">idx</span><span class="p">][</span><span class="s1">&#39;context&#39;</span><span class="p">]</span>
    <span class="n">question</span> <span class="o">=</span> <span class="n">squad_ds</span><span class="p">[</span><span class="s1">&#39;validation&#39;</span><span class="p">][</span><span class="n">idx</span><span class="p">][</span><span class="s1">&#39;question&#39;</span><span class="p">]</span>
    <span class="n">student_pipe</span><span class="p">(</span><span class="n">question</span><span class="o">=</span><span class="n">question</span><span class="p">,</span> <span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>CPU times: user 19min 47s, sys: 13.3 s, total: 20min 1s
Wall time: 3min 1s
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>From this example, we see roughly a 2x speedup from using a distilled model with less than 3% drop in Exact Match / F1-score!</p>

</div>
</div>
</div>
</div>
 

