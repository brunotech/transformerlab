---

title: Weight Pruning


keywords: fastai
sidebar: home_sidebar

summary: "Masked classes for pruning BERT-like Transformers"
description: "Masked classes for pruning BERT-like Transformers"
nb_path: "02_pruning.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: 02_pruning.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">




 
 
<div id="be5fb403-be8f-4893-9291-bedbef907af9"></div>
<div class="output_subarea output_widget_view ">
<script type="text/javascript">
var element = $('#be5fb403-be8f-4893-9291-bedbef907af9');
</script>
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "b5d9bf57e1b84628bb202a44bc19252d"}
</script>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>
</pre>
</div>
</div>

<div class="output_area">




 
 
<div id="136ddb12-cad3-4b47-88b5-674d0b85f4de"></div>
<div class="output_subarea output_widget_view ">
<script type="text/javascript">
var element = $('#136ddb12-cad3-4b47-88b5-674d0b85f4de');
</script>
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "d07edcc83a0244d3b0829732523d2c77"}
</script>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The classes in this module are adapted from <a href="https://twitter.com/SanhEstPasMoi?s=20">Victor Sanh's</a> implementation of <a href="https://arxiv.org/abs/2005.07683"><em>Movement Pruning: Adaptive Sparsity by Fine-Tuning</em></a> in the <code>examples/research_projects</code> of the <code>transformers</code> repository. The main changes are as follows:</p>
<ul>
<li>To make these classes compatible with v4 of <code>transformers</code> we have replaced all instances of <code>BertLayerNorm</code> with <code>torch.nn.LayerNorm</code></li>
<li>In the <code>forward</code> method of <a href="/transformerlab/pruning.html#TopKBinarizer"><code>TopKBinarizer</code></a>, we check whether <code>threshold</code> is a float or a list, since the latter occurs in the <code>datasets</code> format.</li>
</ul>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Masked-versions-of-BERT">Masked versions of BERT<a class="anchor-link" href="#Masked-versions-of-BERT"> </a></h3><p>In order to compute the adaptive mask during pruning, a special set of "masked" BERT classes is required to account for sparsity in the model's weights. <a href="https://twitter.com/madlag?s=20">Fran√ßois Lagunas</a> is planning to release a <a href="https://github.com/huggingface/pytorch_block_sparse">generic version</a> of these classes around March 2021, so we should consider using those when they become available.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="MaskedBertConfig" class="doc_header"><code>class</code> <code>MaskedBertConfig</code><a href="https://github.com/lewtun/transformerlab/tree/master/transformerlab/pruning.py#L21" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>MaskedBertConfig</code>(<strong><code>vocab_size</code></strong>=<em><code>30522</code></em>, <strong><code>hidden_size</code></strong>=<em><code>768</code></em>, <strong><code>num_hidden_layers</code></strong>=<em><code>12</code></em>, <strong><code>num_attention_heads</code></strong>=<em><code>12</code></em>, <strong><code>intermediate_size</code></strong>=<em><code>3072</code></em>, <strong><code>hidden_act</code></strong>=<em><code>'gelu'</code></em>, <strong><code>hidden_dropout_prob</code></strong>=<em><code>0.1</code></em>, <strong><code>attention_probs_dropout_prob</code></strong>=<em><code>0.1</code></em>, <strong><code>max_position_embeddings</code></strong>=<em><code>512</code></em>, <strong><code>type_vocab_size</code></strong>=<em><code>2</code></em>, <strong><code>initializer_range</code></strong>=<em><code>0.02</code></em>, <strong><code>layer_norm_eps</code></strong>=<em><code>1e-12</code></em>, <strong><code>pad_token_id</code></strong>=<em><code>0</code></em>, <strong><code>pruning_method</code></strong>=<em><code>'topK'</code></em>, <strong><code>mask_init</code></strong>=<em><code>'constant'</code></em>, <strong><code>mask_scale</code></strong>=<em><code>0.0</code></em>, <strong>**<code>kwargs</code></strong>) :: <code>PretrainedConfig</code></p>
</blockquote>
<p>A class replicating the <code>~transformers.BertConfig</code> with additional parameters for pruning/masking configuration.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="MaskedBertPreTrainedModel" class="doc_header"><code>class</code> <code>MaskedBertPreTrainedModel</code><a href="https://github.com/lewtun/transformerlab/tree/master/transformerlab/pruning.py#L67" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>MaskedBertPreTrainedModel</code>(<strong><code>config</code></strong>:<code>PretrainedConfig</code>, <strong>*<code>inputs</code></strong>, <strong>**<code>kwargs</code></strong>) :: <code>PreTrainedModel</code></p>
</blockquote>
<p>An abstract class to handle weights initialization and
a simple interface for downloading and loading pretrained models.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="MaskedBertModel" class="doc_header"><code>class</code> <code>MaskedBertModel</code><a href="https://github.com/lewtun/transformerlab/tree/master/transformerlab/pruning.py#L90" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>MaskedBertModel</code>(<strong><code>config</code></strong>) :: <a href="/transformerlab/pruning.html#MaskedBertPreTrainedModel"><code>MaskedBertPreTrainedModel</code></a></p>
</blockquote>
<p>The <a href="/transformerlab/pruning.html#MaskedBertModel"><code>MaskedBertModel</code></a> class replicates the :class:<code>~transformers.BertModel</code> class
and adds specific inputs to compute the adaptive mask on the fly.
Note that we freeze the embeddings modules from their pre-trained values.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="BertEmbeddings" class="doc_header"><code>class</code> <code>BertEmbeddings</code><a href="https://github.com/lewtun/transformerlab/tree/master/transformerlab/pruning.py#L249" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>BertEmbeddings</code>(<strong><code>config</code></strong>) :: <code>Module</code></p>
</blockquote>
<p>Construct the embeddings from word, position and token_type embeddings.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="BertEncoder" class="doc_header"><code>class</code> <code>BertEncoder</code><a href="https://github.com/lewtun/transformerlab/tree/master/transformerlab/pruning.py#L289" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>BertEncoder</code>(<strong><code>config</code></strong>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="MaskedBertForQuestionAnswering" class="doc_header"><code>class</code> <code>MaskedBertForQuestionAnswering</code><a href="https://github.com/lewtun/transformerlab/tree/master/transformerlab/pruning.py#L336" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>MaskedBertForQuestionAnswering</code>(<strong><code>config</code></strong>) :: <a href="/transformerlab/pruning.html#MaskedBertPreTrainedModel"><code>MaskedBertPreTrainedModel</code></a></p>
</blockquote>
<p>An abstract class to handle weights initialization and
a simple interface for downloading and loading pretrained models.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="BertLayer" class="doc_header"><code>class</code> <code>BertLayer</code><a href="https://github.com/lewtun/transformerlab/tree/master/transformerlab/pruning.py#L398" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>BertLayer</code>(<strong><code>config</code></strong>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="BertAttention" class="doc_header"><code>class</code> <code>BertAttention</code><a href="https://github.com/lewtun/transformerlab/tree/master/transformerlab/pruning.py#L434" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>BertAttention</code>(<strong><code>config</code></strong>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="BertSelfAttention" class="doc_header"><code>class</code> <code>BertSelfAttention</code><a href="https://github.com/lewtun/transformerlab/tree/master/transformerlab/pruning.py#L486" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>BertSelfAttention</code>(<strong><code>config</code></strong>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="MaskedLinear" class="doc_header"><code>class</code> <code>MaskedLinear</code><a href="https://github.com/lewtun/transformerlab/tree/master/transformerlab/pruning.py#L583" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>MaskedLinear</code>(<strong><code>in_features</code></strong>:<code>int</code>, <strong><code>out_features</code></strong>:<code>int</code>, <strong><code>bias</code></strong>:<code>bool</code>=<em><code>True</code></em>, <strong><code>mask_init</code></strong>:<code>str</code>=<em><code>'constant'</code></em>, <strong><code>mask_scale</code></strong>:<code>float</code>=<em><code>0.0</code></em>, <strong><code>pruning_method</code></strong>:<code>str</code>=<em><code>'topK'</code></em>) :: <code>Linear</code></p>
</blockquote>
<p>Fully Connected layer with on the fly adaptive mask.
If needed, a score matrix is created to store the importance of each associated weight.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="BertSelfOutput" class="doc_header"><code>class</code> <code>BertSelfOutput</code><a href="https://github.com/lewtun/transformerlab/tree/master/transformerlab/pruning.py#L661" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>BertSelfOutput</code>(<strong><code>config</code></strong>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="BertIntermediate" class="doc_header"><code>class</code> <code>BertIntermediate</code><a href="https://github.com/lewtun/transformerlab/tree/master/transformerlab/pruning.py#L682" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>BertIntermediate</code>(<strong><code>config</code></strong>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="BertOutput" class="doc_header"><code>class</code> <code>BertOutput</code><a href="https://github.com/lewtun/transformerlab/tree/master/transformerlab/pruning.py#L703" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>BertOutput</code>(<strong><code>config</code></strong>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="BertPooler" class="doc_header"><code>class</code> <code>BertPooler</code><a href="https://github.com/lewtun/transformerlab/tree/master/transformerlab/pruning.py#L724" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>BertPooler</code>(<strong><code>config</code></strong>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="TopKBinarizer" class="doc_header"><code>class</code> <code>TopKBinarizer</code><a href="https://github.com/lewtun/transformerlab/tree/master/transformerlab/pruning.py#L739" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>TopKBinarizer</code>() :: <code>Function</code></p>
</blockquote>
<p>Top-k Binarizer.
Computes a binary mask M from a real value matrix S such that <code>M_{i,j} = 1</code> if and only if <code>S_{i,j}</code>
is among the k% highest values of S.</p>
<p>Implementation is inspired from:
    <a href="https://github.com/allenai/hidden-networks">https://github.com/allenai/hidden-networks</a>
    What's hidden in a randomly weighted neural network?
    Vivek Ramanujan<em>, Mitchell Wortsman</em>, Aniruddha Kembhavi, Ali Farhadi, Mohammad Rastegari</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

</div>
 

<script type="application/vnd.jupyter.widget-state+json">
{"state": {"02141e535b7a41d592f5e8f3caa6a1c0": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "8b725fa26c944cbc85c5d897fae188bd": {"model_name": "ProgressStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": "initial"}}, "9d0ef590a95d448ba49665d268330a67": {"model_name": "FloatProgressModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "Downloading: ", "description_tooltip": null, "layout": "IPY_MODEL_02141e535b7a41d592f5e8f3caa6a1c0", "max": 1616.0, "min": 0.0, "orientation": "horizontal", "style": "IPY_MODEL_8b725fa26c944cbc85c5d897fae188bd", "value": 1616.0}}, "f8135bb48438445ea0bbad43238888c1": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "3a6f2c23fc1549ac93aaee5759eca942": {"model_name": "DescriptionStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "20eedd8f88694ec2aadc90c2aada0ea0": {"model_name": "HTMLModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_f8135bb48438445ea0bbad43238888c1", "placeholder": "\u200b", "style": "IPY_MODEL_3a6f2c23fc1549ac93aaee5759eca942", "value": " 4.02k/? [00:00&lt;00:00, 6.16kB/s]"}}, "cae623b1bc7b405f870b9376eb8b879d": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "b5d9bf57e1b84628bb202a44bc19252d": {"model_name": "HBoxModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_9d0ef590a95d448ba49665d268330a67", "IPY_MODEL_20eedd8f88694ec2aadc90c2aada0ea0"], "layout": "IPY_MODEL_cae623b1bc7b405f870b9376eb8b879d"}}, "889b409601694f0581f32100cf1e5d21": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "850c5cab97e1441cb087e9a768e98883": {"model_name": "ProgressStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": "initial"}}, "71b70130b74b4dd881abbf986e8e5600": {"model_name": "FloatProgressModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "Downloading: ", "description_tooltip": null, "layout": "IPY_MODEL_889b409601694f0581f32100cf1e5d21", "max": 1195.0, "min": 0.0, "orientation": "horizontal", "style": "IPY_MODEL_850c5cab97e1441cb087e9a768e98883", "value": 1195.0}}, "093fdd8e4524409b99b1cb118ce3165d": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "fd294e018e434395bc67281e6ab89fb8": {"model_name": "DescriptionStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "f42804fec4ce41f38a3fb55742970b88": {"model_name": "HTMLModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_093fdd8e4524409b99b1cb118ce3165d", "placeholder": "\u200b", "style": "IPY_MODEL_fd294e018e434395bc67281e6ab89fb8", "value": " 3.35k/? [00:00&lt;00:00, 31.7kB/s]"}}, "6f70b6fd3f9d47e0bb0ffbce0060300a": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "d07edcc83a0244d3b0829732523d2c77": {"model_name": "HBoxModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_71b70130b74b4dd881abbf986e8e5600", "IPY_MODEL_f42804fec4ce41f38a3fb55742970b88"], "layout": "IPY_MODEL_6f70b6fd3f9d47e0bb0ffbce0060300a"}}}, "version_major": 2, "version_minor": 0}
</script>

